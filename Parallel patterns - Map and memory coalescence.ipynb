{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Patterns - Map\n",
    "\n",
    "The simplest parallel patterns is probably map. The idea is that same operation (or a function) is applied to every element of an input array. Examples of map can be:\n",
    "* vector scaling \n",
    "* vector addition\n",
    "\n",
    "Map is [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) problem - it is straighforward to parallelize this algorithm, with no communication required between the threads. In Computer graphics pixel shading or ray tracing is embarrasingly parallel because each pixel can be easily processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQbNSmPHSnKybcXyErXr9Tz2nqZf80bXCWvaL0x1GEq0MvsWgNJobxgw8Qp62hl_INBwdo2cnaAhVkn/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel compilation in notebooks\n",
    "\n",
    "PyOpenCL added execution and compilation support for kernels from Jupyter notebooks. It is possible to edit and compile kernels directly is a notebook cell.\n",
    "\n",
    "First, load the PyOpenCL IPython extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pyopencl.ipython_ext\n",
    "\n",
    "import os\n",
    "os.environ[\"GPU_DEVICE_ORDINAL\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of the notebook support you have to give Context a name 'ctx' or 'cl_ctx'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyopencl as cl\n",
    "\n",
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "\n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the kernel directly from notebook. Add the kernel magic '%%cl_kernel' at the beginning, then pass required [compilation options](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/clBuildProgram.html).\n",
    "\n",
    "It's sufficient to declare a kernel here - in jupyter notebook cell. Notice that the cell execution take slightly longer time than just declaring a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  buffer_gpu[gid] = buffer_gpu[gid] * multiplier;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling arbitrary dataset sizes\n",
    "\n",
    "The number of threads in a group applies to all work groups - all work groups must be of the same size. This does not always align with the dataset size being processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRpVhN7h5X-1EFA1wH7ipQJcbBw4iWp4Q65mklz_ZZWuaDrJJo8C0Y_4L4QmPzTsQ4TBPcDOGVkwX3x/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset smaller than number of threads\n",
    "\n",
    "To handle situations with less data than threads we can:\n",
    "* pass number of elements in the dataset\n",
    "* add a check to the kernel.\n",
    "\n",
    "In the case of vector scaling kernel the code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier, int number_of_elements)\n",
    "{\n",
    "    const int gid = get_global_id(0);\n",
    "    if (gid < number_of_elements) {\n",
    "        buffer_gpu[gid] = buffer_gpu[gid] * 2;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset bigger than the number of threads\n",
    "\n",
    "With larger datasets handling one data cell per thread requires creation of huge amounts on work groups. This may not always be the most efficient way to solve a problem. \n",
    "\n",
    "A common way to reduce the amount of threads is to keep the thread alive after it processes a cell. When a thread is finished processing a cell it will then jump to process the next element.\n",
    "\n",
    "There are various options how to divide the entire dataset. An efficient solution from memory-accesses point of view is so called **grid-stride loop**. In this solution we use a for-loop in which we jump in each iteration by the size of the grid.\n",
    "\n",
    "Size of the grid can be queried or calculated in the following ways:\n",
    "* [get_global_size](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_global_size.html)\n",
    "* [get_local_size](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_local_size.html) x [get_num_groups](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_num_groups.html) - multiply work group size by number of work groups. In frameworks global size is not available so that's the reason to calculate the threads amount in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier, int number_of_elements)\n",
    "{\n",
    "    const int gid = get_global_id(0);\n",
    "    const int grid_stride = get_global_size(0);\n",
    "\n",
    "    for (int i = gid; i < number_of_elements; i += grid_stride) \n",
    "    {\n",
    "        buffer_gpu[i] = buffer_gpu[i] * multiplier;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of frameworks where get_global_id or get_global_size intrinsic functions are not available global thread index can be calculated in the way similar to this (where the first four variables may be build-in ones - like in CUDA):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier, int number_of_elements)\n",
    "{\n",
    "    const uint3 threadId = (uint3)(get_local_id(0), get_local_id(1), get_local_id(2));\n",
    "    const uint3 groupId = (uint3)(get_group_id(0), get_group_id(1), get_group_id(2));\n",
    "    const uint3 groupDim = (uint3)(get_local_size(0), get_local_size(1), get_local_size(2));\n",
    "    const uint3 gridDim = (uint3)(get_num_groups(0), get_num_groups(1), get_num_groups(2));\n",
    "    \n",
    "    const int gid = threadId.x + groupDim.x * groupId.x;\n",
    "    const uint3 grid_stride = gridDim.x * groupDim.x;\n",
    "    \n",
    "    for (int i = gid; i < number_of_elements; i+= grid_stride.x) \n",
    "    {\n",
    "        buffer_gpu[i] = buffer_gpu[i] * 2;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercies - vector addition\n",
    "\n",
    "Now that you know how handle data GPU with varying sizes you are ready to write a proper GPU application.\n",
    "\n",
    "Your task is to accelerate vector addition with PyOpenCL.\n",
    "\n",
    "The code below multiplies the vector by 2. Extend the code to:\n",
    "1. Fix the code calculate the following formula:\n",
    "\n",
    "$$\n",
    "res = 2 a + b\n",
    "$$\n",
    "\n",
    "    - accept two input arrays 'a' and 'b'\n",
    "    - write the results so that the input arrays are not overwritten\n",
    "\n",
    "2. Improve the code to:\n",
    "\n",
    "    - handle arbitrary vectors size\n",
    "    - reuse the threads in work group so that you don't have to create as many threads as input items\n",
    "    - take care of reading data out of bounds\n",
    "    - take care of buffers' access parameters - read only, write only etc.\n",
    "\n",
    "3. Fix performance\n",
    "\n",
    "    - experiment with work group size and the amount of work groups\n",
    "    - take care of memory coalescence\n",
    "    - measure performance of GPU execution time\n",
    "    - measure performance of CPU time\n",
    "\n",
    "To create an uninitialized output buffer you need to pass a size in bytes, which can be retrieved like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "some_array = np.zeros(8).astype(np.float32)\n",
    "result_gpu = cl.Buffer(ctx, flags.WRITE_ONLY, some_array.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU profiling helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def profile_cpu(function, n, *args):\n",
    "    \"\"\"\n",
    "    This function profiles other functions:\n",
    "        function - any function to be profiled\n",
    "        n - number of times a function will be rerun - the more times you run it the more stable results you get,\n",
    "            but the more time it will take to profile\n",
    "        args - variable list of arguments that will be passed to a profiled function\n",
    "    \"\"\"\n",
    "    times = np.zeros(n)\n",
    "    total_ms = 0\n",
    "    value = 0\n",
    "    for i in range(n):\n",
    "        start = time()\n",
    "        value = function(*args)\n",
    "        end = time()\n",
    "        elapsed = (end - start) * 1e3\n",
    "        times[i] = elapsed\n",
    "\n",
    "    avg_ms = np.mean(times)\n",
    "    median_ms = np.median(times)\n",
    "    variance = np.var(times)\n",
    "    std = np.std(times)\n",
    "\n",
    "    print(f\"{function.__name__} took on average {avg_ms:.4f} ms, with median {median_ms:.4f} ms, variance {variance:.4f} ms, standard deviation {std:.4f} ms.\")        \n",
    "    return value, avg_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU profiling helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def profile_gpu(function, n, queue, global_size, local_size, *args):\n",
    "    times = np.zeros(n)\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    \n",
    "    for i in range(n):\n",
    "        e = function(queue, global_size, local_size, *args)\n",
    "        e.wait()\n",
    "        elapsed = (e.profile.end - e.profile.start) * 1e-6\n",
    "        times[i] = elapsed\n",
    "\n",
    "    avg_ms = np.mean(times)\n",
    "    median_ms = np.median(times)\n",
    "    variance = np.var(times)\n",
    "    std = np.std(times)\n",
    "\n",
    "    print(f\"{function.function_name} took on average {avg_ms:.4f} ms, with median {median_ms:.4f} ms, variance {variance:.4f} ms, standard deviation {std:.4f} ms.\")\n",
    "    \n",
    "    return avg_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void compute_linear_equations_gpu(__global int *a)\n",
    "{\n",
    "    const uint gid = get_global_id(0);\n",
    "\n",
    "    a[gid] = 5 * a[gid];\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel name is injected into user namespace so use the function just by it's name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N=np.int32(10000000)\n",
    "\n",
    "h_a = np.arange(0, N).astype(np.int32)\n",
    "h_b = np.arange(1, N + 1).astype(np.int32)\n",
    "\n",
    "d_a = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_a)\n",
    "\n",
    "local_work_size = (32, )\n",
    "global_work_size = (32, )\n",
    "\n",
    "gpu_time_ms = profile_gpu(compute_linear_equations_gpu, \n",
    "                          20, \n",
    "                          queue, \n",
    "                          global_work_size, \n",
    "                          local_work_size,\n",
    "                          d_a\n",
    "                          )\n",
    "\n",
    "h_res = np.zeros(N).astype(np.int32)\n",
    "\n",
    "_ = cl.enqueue_copy(queue, h_res, d_a)\n",
    "\n",
    "def compute_linear_equations_cpu(a, b):\n",
    "    return 2 * a + b\n",
    "\n",
    "numpy_res, numpy_avg_ms = profile_cpu(compute_linear_equations_cpu, 20, h_a, h_b)\n",
    "np.testing.assert_equal(h_res, numpy_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [solution](./map_solution.txt) if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
