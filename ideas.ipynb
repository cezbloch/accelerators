{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-Reduce\n",
    "\n",
    "* Polynomial calculation\n",
    "* Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe how a scheduler works\n",
    "\n",
    "Scheduler has\n",
    "* a queue of tasks\n",
    "\n",
    "takes into account:\n",
    "* has to fit into shared memory\n",
    "* has to fit into registers - if not it has to spill registers to higher level memory\n",
    "* tries to utilize available cores/SIMDs\n",
    "* compiled kernel size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction\n",
    "* suitable for assosiative operators\n",
    "* floating point operation order matters - not assosiative\n",
    "* uses more memory - O(n) - to store intermediate results, serial version uses registers - O(1)\n",
    "* O(log(n)) iterations\n",
    "* can be composed of parallel block - each of them with serial loop inside\n",
    "- in GPU each threads can access part of a cache line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levels of parallelism\n",
    "\n",
    "* machine level\n",
    "* device level\n",
    "* processor level\n",
    "* thread level\n",
    "* vectorisation\n",
    "* instruction level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Virtual Memory\n",
    "\n",
    "If OpenCL 2.0 is available it is possible to use one pointer for both host and gpu memory - similarly to CUDA managed memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenCL to CUDA dictionary\n",
    "\n",
    "\n",
    "\"OpenCL CUDA\n",
    "Grid Grid\n",
    "Work Group Block\n",
    "Work Item Thread\n",
    "kernel global\n",
    "global device\n",
    "local shared\n",
    "private local\n",
    "imagend t texture<type, n, ...>\n",
    "barrier(LMF) syncthreads()\n",
    "get local id(012) threadIdx.xyz\n",
    "get group id(012) blockIdx.xyz\n",
    "get global id(012) â€“ (reimplement)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Matrix Multiplication\n",
    "\n",
    "So far we have used only one dimension of global and local work group size. For some problems it may be more convenient to think about it in two or three dimensions. Global and local work group sizes can have up to three dimensions. This limitation originates in computer graphics from which compute kernels evolved. The other name for compute kernels is shader - because the first programmable graphics pipeline stage was used for 'shading' - or illuminating - pixels.\n",
    "\n",
    "Your task in this exercise is to move the implementation of square matrix multiplication from python to GPU. Square matrix has the same number of rows and columns.\n",
    "* add a kernel that will correctly multiply two matrices\n",
    "* use two dimensions when addressing threads in GPU - for local and global size\n",
    "* compare execution with with the CPU version - think about what you are comparing\n",
    "\n",
    "Write you code in [this](./matrix_multiplication.py) file. To check your code execute the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python matrix_multiplication.py --n 1024 --threads 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [solution](./matrix_multiplication_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel OneAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-5bf5cd1824cb>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-5bf5cd1824cb>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    namespace sycl = cl::sycl;\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#include <vector>\n",
    "#include <CL/sycl.hpp>\n",
    "\n",
    "#define SIZE 1024\n",
    "\n",
    "namespace sycl = cl::sycl;\n",
    "\n",
    "int main() {\n",
    "  std::array<int, SIZE> a, b, c;\n",
    "\n",
    "  for (int i = 0; i<SIZE; ++i) {\n",
    "    a[i] = i;\n",
    "    b[i] = -i;\n",
    "    c[i] = i;\n",
    "  }\n",
    "\n",
    "  {\n",
    "  sycl::range<1> a_size{SIZE};\n",
    "\n",
    "  auto platforms = sycl::platform::get_platforms();\n",
    " \n",
    " for (auto &platform : platforms) {\n",
    "\n",
    "    std::cout << \"Platform: \"\n",
    "      << platform.get_info<sycl::info::platform::name>()\n",
    "      << std::endl;\n",
    "\n",
    "\n",
    "    auto devices = platform.get_devices();\n",
    "    for (auto &device : devices ) {\n",
    "      std::cout << \"  Device: \"\n",
    "        << device.get_info<sycl::info::device::name>()\n",
    "        << std::endl;\n",
    "    }\n",
    "\n",
    "  }\n",
    "\n",
    "  sycl::default_selector device_selector;\n",
    "  sycl::queue d_queue(device_selector);\n",
    "\n",
    "  sycl::buffer<int, 1>  a_device(a.data(), a_size);\n",
    "  sycl::buffer<int, 1>  b_device(b.data(), a_size);\n",
    "  sycl::buffer<int, 1>  c_device(c.data(), a_size);\n",
    "\n",
    "  d_queue.submit([&](sycl::handler &cgh) {\n",
    "    auto c_res = c_device.get_access<sycl::access::mode::write>(cgh);\n",
    "    auto a_in = a_device.get_access<sycl::access::mode::read>(cgh);\n",
    "    auto b_in = b_device.get_access<sycl::access::mode::read>(cgh);\n",
    "\n",
    "    cgh.parallel_for<class ex1>(a_size,[=](sycl::id<1> idx) {\n",
    "      c_res[idx] = a_in[idx] + b_in[idx];\n",
    "    });\n",
    "\n",
    "  });\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
