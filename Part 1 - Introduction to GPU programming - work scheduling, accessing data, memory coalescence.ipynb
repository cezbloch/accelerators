{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyOpenCL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who makes GPUs?\n",
    "\n",
    "Many of us are aware that the following vendors produce graphics cards for desktop PCs and laptops:\n",
    "* Intel\n",
    "* Nvidia\n",
    "* AMD\n",
    "\n",
    "There is a growing and interesting market in mobile GPUs in our phones represented by the following brands:\n",
    "* Adreno - Qualcomm\n",
    "* Mali - ARM\n",
    "* PowerVR - Imagination\n",
    "\n",
    "### What tools are there to program GPUs?\n",
    "\n",
    "There are many frameworks which enable developers to program GPUs. Some of the most popular are listed below:\n",
    "\n",
    "* OpenCL by Khronos organization (cross platform)\n",
    "* Vulkan by Khronos - getting increasingly popular (cross platform)\n",
    "* DirectCompute - part of DirectX by Microsoft (Windows only)\n",
    "* CUDA by NVidia (NVidia only)\n",
    "* Metal by Apple (Mac only)\n",
    "\n",
    "Some are available only on specific hardware (like CUDA) or operating systems (like Metal or DirectX).\n",
    "\n",
    "### Why OpenCL?\n",
    "\n",
    "OpenCL is available on most platforms - runs on all three big desktop platforms as well as on Android devices. It's well established in the market, widely used and with good documentation. \n",
    "\n",
    "Some useful vendor extensions are available only through OpenCL. \n",
    "\n",
    "### Why Python for OpenCL?\n",
    "Python is a great language for learning because of its simplicity. The goal of this workshop is to focus your attention on GPU concepts and algorithms rather than spending hours settings up complicated low level code.\n",
    "\n",
    "PyOpenCL is complete Python OpenCL API made easier. Underneath it uses C++ so there is no speed compromise. Comes under MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host code and device code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will dive straight into the code skipping the litany of reasons why GPUs should be used for General Purpose computing.\n",
    "\n",
    "When working with GPUs we write two types of code:\n",
    "* host code - it can be thought as a boilerplate code - it is executed on CPU. In case of this workshop it is the Python code - with other GPU frameworks like CUDA, OpenCL or DirectX it will most likely be C++. Host side CPU code sets up the necessary buffers, copies data to GPU memory and back and tells GPU what to do in which order.\n",
    "* device code - GPU code - things that are executed on GPU. These are often relatively small pieces of code which are executed many times but with different data. The functions executed on GPU are called kernels or shaders.\n",
    "\n",
    "Below are slides with very high level view of relation between CPU and GPU work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS2_z7au4f9L0BmsYorl_CEdFP-ec6pTCqREdGiRqCvpeNuTG8bm7VHMEfU2tYtiA2cNEkIedixJIqk/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeeHive specific setup\n",
    "\n",
    "If you are working on a BeeHive cluster and share a GPU with other people the lines below are neede to make the notebooks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GPU_DEVICE_ORDINAL\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start coding\n",
    "\n",
    "First thing to do is to import PyOpenCL library. When this succeeds it is a confirmation that PyOpenCL is installed on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn a bit more about the platform we are going to work on. We will use [get_platforms](https://documen.tician.de/pyopencl/runtime_platform.html?highlight=get_platforms#pyopencl.get_platforms) function to get the list of available OpenCL platforms. Examples of platforms include:\n",
    "* Nvidia CUDA\n",
    "* AMD Accelerated Parallel Processing\n",
    "* Intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in cl.get_platforms():\n",
    "    print(f\"profile = {p.profile}, name = {p.name}, version = {p.version}, vendor = {p.vendor}\")\n",
    "    print('\\n\\tPlatform Extensions: ')\n",
    "    for e in p.extensions.split():\n",
    "        print(f\"\\t\\t{e}\")\n",
    "        \n",
    "    devices = p.get_devices(cl.device_type.ALL)\n",
    "\n",
    "    print('\\n\\tAvailable devices: ')\n",
    "    if not devices:\n",
    "        print('\\t\\tNone')\n",
    "\n",
    "    for dev in devices:\n",
    "        indent = '\\t\\t'\n",
    "        print(indent + '{} ({})'.format(dev.name, dev.vendor))\n",
    "\n",
    "        indent = '\\t\\t\\t'\n",
    "        flags = [('Version', dev.version),\n",
    "                 ('Type', cl.device_type.to_string(dev.type)),\n",
    "                 #('Extensions', str(dev.extensions.strip().split(' '))),\n",
    "                 ('Memory (global)', str(dev.global_mem_size)),\n",
    "                 ('Memory (local)', str(dev.local_mem_size)),\n",
    "                 ('Address bits', str(dev.address_bits)),\n",
    "                 ('Max work item dims', str(dev.max_work_item_dimensions)),\n",
    "                 ('Max work group size', str(dev.max_work_group_size)),\n",
    "                 ('Max compute units', str(dev.max_compute_units)),\n",
    "                 ('Driver version', dev.driver_version),\n",
    "                 ('Image support', str(bool(dev.image_support))),\n",
    "                 ('Little endian', str(bool(dev.endian_little))),\n",
    "                 ('Device available', str(bool(dev.available))),\n",
    "                 ('Compiler available', str(bool(dev.compiler_available)))]        \n",
    "        [print(indent + '{0:<25}{1:<10}'.format(name + ':', flag)) for name, flag in flags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use 'clinfo' application to get detailed information about your platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!clinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create open CL context and command queue.\n",
    "Context is an abstraction in which we execute kernels, it isolates the resources you are using. It can contain many devices present in a system. When you exit the application the resources assigned to that context are deleted and cleared when context is destroyed. \n",
    "\n",
    "We will use a somewhat strange function [create_some_context](https://documen.tician.de/pyopencl/runtime_platform.html?highlight=get_platforms#pyopencl.create_some_context) which as the name suggests \"somehow\" creates context. The way how it is done and what it includes is up to a vendor. In case of NVidia it creates context with only one GPU - even if there are multiple GPUs present.\n",
    "\n",
    "Command Queue is a queue to which you schedule different tasks. In a simple case there will be one queue. To overlap some work we can create multiple queues. GPU picks the work from these queues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()    \n",
    "queue = cl.CommandQueue(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating context we can query how many devices are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create context in a predictable way we can use Context constructor which takes arguments:\n",
    "* [device type](https://documen.tician.de/pyopencl/runtime_const.html#device_type) - CPU, GPU, ALL - OpenCL can also be run on FPGAs\n",
    "* properties - where we can pass a platform\n",
    "\n",
    "How many devices are available now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello GPU World\n",
    "\n",
    "Kernel is a function that will run in GPU. It will be executed many times with different data in each thread.\n",
    "\n",
    "The kernel below is named 'hello_gpu'. It takes no arguments.\n",
    "\n",
    "[__kernel](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/functionQualifiers.html) qualifier indicates that a function can be executed on an OpenCL device.\n",
    "\n",
    "The return value of OpenCL kernels has to be void, because they don't return any value. In practical applications the kernel is executed multiple times; there is no single value to return from the function itself. Though data can be returned through buffers which we will cover later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void hello_gpu()\n",
    "{\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Calling a kernel function](https://documen.tician.de/pyopencl/runtime_program.html#pyopencl.Program.kernel_name) on program object schedules the execution of kernel on GPU. Work is scheduled on a queue that is passed as an argument. Processing on a GPU happens asynchronously with respect to the CPU code.\n",
    "\n",
    "One way of running work on GPU is calling a kernel name on a program object. It takes the following arguments:\n",
    "* queue - on which the work will be scheduled\n",
    "* global work size - for now we pass a tuple of 1\n",
    "* local work size - for now we pass a tuple of 1\n",
    "* arguments that will be passed to kernel function - in this case there are no extra arguments\n",
    "\n",
    "The return value is an event associated with the work. After scheduling the execution of work on the GPU we wait for the kernel to finish, since the work is done asynchronously with respect to CPU python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = hello_gpu(queue, (1,), (1,))\n",
    "\n",
    "event.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a code that compiles and executes but does nothing yet. \n",
    "\n",
    "## Work division\n",
    "\n",
    "Before we pass some data for processing it's neccessary to understand how to create multiple threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRcAADTeaEmIr2nO_XGEnelxWP3OQjtRDEWxrZr08sayWBmx7nJyiGfSQcOKEIPcawr6ptd5LrNGXEn/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing data in kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRxua8PCOImOODpVPZRFTtUvQKnTHn_kcRE44M7aXjzoZPUJLZtlozxHk7Of002F4fmUu3yUpWs2eRR/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you will see an example of a kernel that accesses data from a buffer. It will multiply every value in the buffer by a constant and will store it in the same memory cell.\n",
    "\n",
    "First we create a regular, CPU visible numpy array. By convention handles to objects will be prefixed with:\n",
    "* h_ - on the host - CPU buffers\n",
    "* d_ - on device side - GPU buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 8\n",
    "h_buffer = np.arange(0, N).astype(np.int32)\n",
    "\n",
    "h_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs often have their own memory on the Graphics Card. To perform operations on data in GPU we have to copy the memory from CPU visible RAM to GPU. This can done when creating [Buffer](https://documen.tician.de/pyopencl/runtime_memory.html?highlight=buffer#pyopencl.Buffer) object with arguments:\n",
    "* context - Context that we have previously created\n",
    "* [flags](https://documen.tician.de/pyopencl/runtime_const.html?highlight=mem_flags#mem_flags) argument we passed tell that:\n",
    " - READ_WRITE - we will be reading the value from the buffer and writing back - so the buffer should be readable and writable\n",
    " - COPY_HOST_PTR - the memory in GPU should be filled with data provided in 'hostbuf' argument\n",
    "* Argument hostbuf is assigned a CPU visible memory we have created with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "\n",
    "d_input_buffer = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_buffer)\n",
    "d_output_buffer = cl.Buffer(ctx, flags.WRITE_ONLY, h_buffer.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Scale kernel\n",
    "The kernel below is named 'scale_vector'. It takes two arguments:\n",
    "* read only pointer to an input buffer\n",
    "* writable pointer to output buffer\n",
    "* multiplier - read-only constant value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void scale_vector(__global const int *input_buffer, \n",
    "                           __global int *output_buffer, \n",
    "                           const int multiplier)\n",
    "{\n",
    "  int gid = get_global_id(0);\n",
    "  output_buffer[gid] = input_buffer[gid] * multiplier;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPU there are a few kinds of memory address spaces where data can reside. GPU RAM is a global memory and we use\n",
    "__global - qualifier to state that.\n",
    "Available address space qualifiers are:\n",
    "* [global](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/global.html) - RAM memory - slowest but a lot is available\n",
    "* [local](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/local.html) - shared memory - close to processors - faster but not much available, high usage may decrease performance.\n",
    "* [constant](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/constant.html) - special GPU read only memory, filled from CPU - used for constants\n",
    "* [private](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/private.html) - registers - fastest but little of it is available, low usage is usually good for performance.\n",
    "\n",
    "__kernel qualifier indicates that a function can be executed on an OpenCL device.\n",
    "\n",
    "scale_vector kernel function takes as argument a pointer to an array. Each thread accesses one element from an array and multiplies it by a value. \n",
    "To get the index from which a thread will read the value, we use OpenCL built-in function:\n",
    "\n",
    "[get_global_id](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_global_id.html) - returns the global index of the current thread. Argument with value 0 indicates that we get index on x-axis. Similarly value 1 will return index on y-axis, and value 2 index on z-axis.\n",
    "\n",
    "Data is returned from kernels through arguments - in this case by writing the value back the output buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create simple execution configuration with one thread per work group. This is specified by defining local work group size. The amount of work groups is defined in global work size.\n",
    "\n",
    "There will be as many work groups as elements in the arrays.\n",
    "\n",
    "We must also define a constant that will be passed to a kernel as a parameter. It has to be a numpy value - otherwise PyOpenCL cannot understand the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_work_size = (1,)\n",
    "global_work_size = (N,)\n",
    "\n",
    "multiplier = np.int32(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments passed to this kernel invocation are:\n",
    "* queue - on which the work will be scheduled.\n",
    "* global work size - stating the total number of threads in a grid. It's the number of independent work groups multiplied by threads amount in work group (by local work size).\n",
    "* local work size - stating how many threads you will have in a work group.\n",
    "* variable amount of arguments that will be passed to kernel function. In case of this kernel there are two arguments:\n",
    " - input buffer\n",
    " - output buffer\n",
    " - multiplier - a numpy constant by which vector will be multiplied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = scale_vector(queue, \n",
    "                     global_work_size, \n",
    "                     local_work_size,\n",
    "                     d_input_buffer,\n",
    "                     d_output_buffer,\n",
    "                     multiplier)\n",
    "\n",
    "event.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of vector multiplication is available in GPU buffer. To access it here in Python code we need to copy it back from GPU memory to CPU. \n",
    "\n",
    "So we need a CPU visible buffer. We create an empty array for that. Then we need to fetch the data back.\n",
    "\n",
    "We can use [enqueue_copy](https://documen.tician.de/pyopencl/runtime_memory.html#pyopencl.enqueue_copy) OpenCL function which schedules a copy from GPU to CPU on the same queue. The function will block by default. If you do not want it to block set is_blocking argument to False. Note that in this case you will have to manually synchronize GPU with CPU at some point. Otherwise your buffer may be in invalid state.\n",
    "\n",
    "To verify if the multiplication was done correctly in the GPU we also multiply the input CPU buffer here. By comparing CPU multipied buffer with GPU multiplied one we know that the processing was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h_result_from_gpu = np.zeros(N).astype(np.int32)\n",
    "cl.enqueue_copy(queue, h_result_from_gpu, d_output_buffer)\n",
    "\n",
    "result_from_cpu = h_buffer * multiplier\n",
    "print(f\"computed in cpu = {result_from_cpu}\")\n",
    "print(f\"computed in gpu = {h_result_from_gpu}\")\n",
    "\n",
    "np.testing.assert_array_equal(result_from_cpu, h_result_from_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Efficient hardware utilization with optimal subgroup size\n",
    "\n",
    "To utilize the underlying hardware efficiently we need to understand basics of underlying hardware.\n",
    "\n",
    "In the example above 'local_work_size' was set to one. This is very not optimal which will be explained in the slides below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQYzwdOhK1JejtcOPk7ua972EVKQ3OluGZ-m9LPl4Uhp0bOK1D_wWWzATBRHa8wc1NR6_P__DFJBmFv/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a hint about the best work group size you can query the compiled kernel for performance hints.\n",
    "Call [get_work_group_info](https://documen.tician.de/pyopencl/runtime_program.html#pyopencl.Kernel.get_work_group_info) on a kernel with kernel_work_group_info parameter value of [PREFERRED_WORK_GROUP_SIZE_MULTIPLE](https://registry.khronos.org/OpenCL/sdk/1.1/docs/man/xhtml/clGetKernelWorkGroupInfo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ctx.get_info(cl.context_info.DEVICES)[0]\n",
    "\n",
    "work_group_multiple = scale_vector.get_work_group_info(cl.kernel_work_group_info.PREFERRED_WORK_GROUP_SIZE_MULTIPLE, device)\n",
    "max_work_size = scale_vector.get_work_group_info(cl.kernel_work_group_info.WORK_GROUP_SIZE, device)\n",
    "local_memory_size = scale_vector.get_work_group_info(cl.kernel_work_group_info.LOCAL_MEM_SIZE, device)\n",
    "private_bytes = scale_vector.get_work_group_info(cl.kernel_work_group_info.PRIVATE_MEM_SIZE, device)\n",
    "\n",
    "print(f\"Preffered work group size multiple = {work_group_multiple}\")\n",
    "print(f\"Maximum work group size = {max_work_size}\")\n",
    "print(f\"Local memory size = {local_memory_size}\")\n",
    "print(f\"Spilled or private memory in bytes = {private_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first printed value is actually subgroup width, the width of your SIMD. Your work group size should be a multiple of it.\n",
    "\n",
    "Later we will profile the kernel defined in the example above. For convevience let's define now a helper function which will measure kernel execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_gpu(function, n, queue, global_size, local_size, *args):\n",
    "    times = np.zeros(n)\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    \n",
    "    for i in range(n):\n",
    "        e = function(queue, global_size, local_size, *args)\n",
    "        e.wait()\n",
    "        elapsed = (e.profile.end - e.profile.start) * 1e-6\n",
    "        times[i] = elapsed\n",
    "\n",
    "    avg_ms = np.mean(times)\n",
    "    median_ms = np.median(times)\n",
    "    variance = np.var(times)\n",
    "    std = np.std(times)\n",
    "    print(f\"{function.function_name} took on average {avg_ms:.4f} ms, with median {median_ms:.4f} ms, variance {variance:.4f} ms, standard deviation {std:.4f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input the previous exercise has just 8 elements and there is no point in doing so little operations in GPU. To get some meaningful results we will increase the size of the input buffer and redefine the variables affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 2**23\n",
    "h_buffer = np.arange(0, N).astype(np.int32)\n",
    "\n",
    "print(f\"Computing {N:,} elements - total of {h_buffer.nbytes:,} bytes.\")\n",
    "\n",
    "flags = cl.mem_flags\n",
    "d_input_buffer = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_buffer)\n",
    "d_output_buffer = cl.Buffer(ctx, flags.WRITE_ONLY, h_buffer.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execution configuration also needs to be updated since we've changed N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_work_size = (1,)\n",
    "global_work_size = (N,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's profile the kernel on the new bigger buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_gpu(scale_vector, 20, \n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            d_input_buffer,\n",
    "            d_output_buffer,\n",
    "            multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to modify the execution configuration defined above to significantly reduce the time taken by the this kernel. Always check if the results after optimizations are still correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_result_from_gpu = np.zeros(N).astype(np.int32)\n",
    "cl.enqueue_copy(queue, h_result_from_gpu, d_output_buffer)\n",
    "\n",
    "result_from_cpu = h_buffer * multiplier\n",
    "print(f\"computed in cpu = {result_from_cpu}\")\n",
    "print(f\"computed in gpu = {h_result_from_gpu}\")\n",
    "\n",
    "np.testing.assert_array_equal(result_from_cpu, h_result_from_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [solution](./waves_solution.txt) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Patterns - Map\n",
    "\n",
    "The simplest parallel patterns is probably map. The idea is that same operation (or a function) is applied to every element of an input array. Examples of map can be:\n",
    "* vector scaling \n",
    "* vector addition\n",
    "\n",
    "Map is [embarrasingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel) problem - it is straighforward to parallelize this algorithm, with no communication required between the threads. In Computer graphics pixel shading or ray tracing is embarrasingly parallel because each pixel can be easily processed separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQbNSmPHSnKybcXyErXr9Tz2nqZf80bXCWvaL0x1GEq0MvsWgNJobxgw8Qp62hl_INBwdo2cnaAhVkn/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define CPU profiling helper function for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "def profile_cpu(function, n, *args):\n",
    "    \"\"\"\n",
    "    This function profiles other functions:\n",
    "        function - any function to be profiled\n",
    "        n - number of times a function will be rerun - the more times you run it the more stable results you get,\n",
    "            but the more time it will take to profile\n",
    "        args - variable list of arguments that will be passed to a profiled function\n",
    "    \"\"\"\n",
    "    times = np.zeros(n)\n",
    "    total_ms = 0\n",
    "    value = 0\n",
    "    for i in range(n):\n",
    "        start = time()\n",
    "        value = function(*args)\n",
    "        end = time()\n",
    "        elapsed = (end - start) * 1e3\n",
    "        times[i] = elapsed\n",
    "\n",
    "    avg_ms = np.mean(times)\n",
    "    median_ms = np.median(times)\n",
    "    variance = np.var(times)\n",
    "    std = np.std(times)\n",
    "\n",
    "    print(f\"{function.__name__} took on average {avg_ms:.4f} ms, with median {median_ms:.4f} ms, variance {variance:.4f} ms, standard deviation {std:.4f} ms.\")        \n",
    "    return value, avg_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Linear Equations\n",
    "\n",
    "It's time to write some kernel yourself. Your task is to modify the code so that multiple simple linear equations are calculated.\n",
    "\n",
    "The code below multiplies the data points from buffer 'a' by 2 and adds bias 'b'. \n",
    "\n",
    "$$\n",
    "res = 2 a + b\n",
    "$$\n",
    "\n",
    "Extend the code below to:\n",
    "* accept two input buffer 'a' and 'b'\n",
    "* accept output buffer 'c'\n",
    "* write the results so that the input arrays are not overwritten\n",
    "* fix execution configuration\n",
    "* set arguments to host function scheduling execution to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = np.int32(2**25)\n",
    "h_a = np.full(N, 1).astype(np.int32)\n",
    "h_b = np.full(N, 2).astype(np.int32)\n",
    "\n",
    "print(f\"Working with {len(h_a):,} elements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required GPU buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "\n",
    "d_a = None\n",
    "# ...\n",
    "d_c = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the kernel below to add elements from two arrays and write the result back to a third array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void compute_linear_equations_gpu(__global int *a)\n",
    "{\n",
    "    // your kernel code goes here\n",
    "}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create appropriate execution configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_work_size = (2,)\n",
    "global_work_size = (128,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute and profile the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_gpu(compute_linear_equations_gpu, \n",
    "            20, \n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            # other arguments\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the resulting array is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h_c = np.zeros(N).astype(np.int32)\n",
    "cl.enqueue_copy(queue, h_c, d_c)\n",
    "\n",
    "def compute_linear_equations_cpu(a, b):\n",
    "    return 2 * a + b\n",
    "\n",
    "numpy_res, numpy_avg_ms = profile_cpu(compute_linear_equations_cpu, 20, h_a, h_b)\n",
    "np.testing.assert_array_equal(numpy_res, h_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [solution](./vector_addition_solution.py) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling arbitrary dataset sizes\n",
    "\n",
    "The number of threads in a group applies to all work groups - all work groups must be of the same size. This does not always align with the dataset size being processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRpVhN7h5X-1EFA1wH7ipQJcbBw4iWp4Q65mklz_ZZWuaDrJJo8C0Y_4L4QmPzTsQ4TBPcDOGVkwX3x/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset smaller than number of threads\n",
    "\n",
    "To handle situations with less data than threads we can:\n",
    "* pass number of elements in the dataset\n",
    "* add a check to the kernel.\n",
    "\n",
    "In the case of vector scaling kernel the code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier, int number_of_elements)\n",
    "{\n",
    "    const int gid = get_global_id(0);\n",
    "    if (gid < number_of_elements) {\n",
    "        buffer_gpu[gid] = buffer_gpu[gid] * 2;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: If you read outside the buffer things can:\n",
    "* go very wrong - your GPU driver can enter invalid state and you will need to restart your computer (or reset GPU in some way)\n",
    "* can go unnoticed - driver may have allocated more memory that you have requested and things will seem to work. But there may be situations where your input data has different size and can cuase crashes in procution.\n",
    "\n",
    "So keep your data access guarded!\n",
    "\n",
    "### Dataset bigger than the number of threads\n",
    "\n",
    "With larger datasets handling one data cell per thread requires creation of huge amounts on work groups. This may not always be the most efficient way to solve a problem. \n",
    "\n",
    "A common way to reduce the number of threads is to keep the thread alive after it has processed a cell. When a thread is finished processing a cell it will then jump to process the next element.\n",
    "\n",
    "There are various options how to divide the entire dataset. An efficient solution from memory-accesses point of view is so called **grid-stride loop**. In this solution we use a for-loop in which we jump in each iteration by the size of the grid.\n",
    "\n",
    "Size of the grid can be queried or calculated in the following ways:\n",
    "* [get_global_size](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_global_size.html)\n",
    "* [get_local_size](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_local_size.html) x [get_num_groups](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/get_num_groups.html) - multiply work group size by number of work groups. In frameworks global size is not available so that's the reason to calculate the threads amount in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void multiply_vector(__global float *buffer_gpu, int multiplier, int number_of_elements)\n",
    "{\n",
    "    const int gid = get_global_id(0);\n",
    "    const int grid_stride = get_global_size(0);\n",
    "\n",
    "    for (int i = gid; i < number_of_elements; i += grid_stride) \n",
    "    {\n",
    "        buffer_gpu[i] = buffer_gpu[i] * multiplier;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - safe vector addition\n",
    "\n",
    "Modify the code from previous exercise to:\n",
    "\n",
    "* handle arbitrary vectors size - take care of reading data out of bounds\n",
    "* add information about number of elements to the kernel\n",
    "* reuse the threads in work group so that you don't have to create as many threads as input items\n",
    "* take care of buffers' access parameters - read only, write only etc.\n",
    "* take care of memory coalescence\n",
    "* experiment with work group size and the amount of work groups\n",
    "* watch GPU execution time\n",
    "    \n",
    "Refer to the [solution](./map_solution.txt) if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
