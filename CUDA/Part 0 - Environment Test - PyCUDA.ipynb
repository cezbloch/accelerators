{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037b4b81",
   "metadata": {},
   "source": [
    "# CUDA Environment Test\n",
    "\n",
    "This notebook mirrors the original PyOpenCL environment test but uses **PyCUDA** to verify your CUDA toolkit + driver + Python stack are correctly installed.\n",
    "\n",
    "Run all cells from top to bottom. The final message should confirm success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf83588",
   "metadata": {},
   "source": [
    "## 1. Imports and setup\n",
    "We import PyCUDA and NumPy. `pycuda.autoinit` creates a context automatically on the first CUDA device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit  # initializes CUDA context\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "print(f'CUDA Driver Version: {drv.get_driver_version()}')\n",
    "device = drv.Context.get_device()\n",
    "print(f'Using device: {device.name()} with compute capability {device.compute_capability()}')\n",
    "print(f'Total Memory (MB): {device.total_memory() / 1024**2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac403628",
   "metadata": {},
   "source": [
    "## 2. Helper: GPU profiling wrapper\n",
    "We'll mimic the earlier timing helper. CUDA events provide timing in milliseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf0f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_gpu(func, n_warmup, n_iters, *kernel_launch_args):\n",
    "    # Warm-up launches (not timed)\n",
    "    for _ in range(n_warmup):\n",
    "        func(*kernel_launch_args)\n",
    "    times = np.zeros(n_iters, dtype=np.float64)\n",
    "    for i in range(n_iters):\n",
    "        start = drv.Event(); end = drv.Event()\n",
    "        start.record()\n",
    "        func(*kernel_launch_args)\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "        times[i] = start.time_till(end)  # ms\n",
    "    print(f'Kernel took on average {times.mean():.4f} ms, median {np.median(times):.4f} ms, std {times.std():.4f} ms over {n_iters} runs.')\n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26730e33",
   "metadata": {},
   "source": [
    "## 3. Host data\n",
    "We allocate large NumPy int32 arrays similar to the OpenCL version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb40610",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**25  # 33,554,432 elements\n",
    "h_a = np.full(N, 1, dtype=np.int32)\n",
    "h_b = np.full(N, 2, dtype=np.int32)\n",
    "print(f'Working with {h_a.size:,} elements consuming {h_a.nbytes/1024**2:.2f} MB per array.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf45e19",
   "metadata": {},
   "source": [
    "## 4. Device allocations and transfers\n",
    "Allocate device memory and transfer host arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ae4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a = drv.mem_alloc(h_a.nbytes)\n",
    "d_b = drv.mem_alloc(h_b.nbytes)\n",
    "d_c = drv.mem_alloc(h_a.nbytes)\n",
    "drv.memcpy_htod(d_a, h_a)\n",
    "drv.memcpy_htod(d_b, h_b)\n",
    "print('Device buffers allocated & data transferred.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb62f6b",
   "metadata": {},
   "source": [
    "## 5. CUDA kernel\n",
    "We reproduce the operation: c[i] = 2*a[i] + b[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = r'''\n",
    "        extern \"C\" __global__ void add_vectors(const int *a, const int *b, int *c, int N) {\n",
    "            int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (gid < N) {\n",
    "                c[gid] = 2 * a[gid] + b[gid];\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "mod = SourceModule(kernel_code, options=['-use_fast_math'])\n",
    "add_vectors = mod.get_function('add_vectors')\n",
    "print('Kernel compiled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fc985",
   "metadata": {},
   "source": [
    "## 6. Execution configuration\n",
    "Choose a block size (work-group size) and derive grid size (number of blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256  # threads per block\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "print(f'Launching with grid_size={grid_size}, block_size={block_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e16da9",
   "metadata": {},
   "source": [
    "## 7. Run & profile kernel\n",
    "We wrap the kernel launch in a lambda for the profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1be7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch = lambda: add_vectors(d_a, d_b, d_c, np.int32(N), block=(block_size,1,1), grid=(grid_size,1))\n",
    "_ = profile_gpu(launch, n_warmup=2, n_iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4dbbd",
   "metadata": {},
   "source": [
    "## 8. Copy back & validate\n",
    "Transfer result back to host and compare with NumPy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_c = np.empty_like(h_a)\n",
    "drv.memcpy_dtoh(h_c, d_c)\n",
    "expected = 2 * h_a + h_b\n",
    "np.testing.assert_array_equal(expected, h_c)\n",
    "print('Validation passed. If this message appears everything worked correctly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45970ea8",
   "metadata": {},
   "source": [
    "## 9. Notes\n",
    "If any cell failed: check that the NVIDIA driver & CUDA toolkit are installed, and that `pycuda` is in your environment (see requirements or install via `pip install pycuda`)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
