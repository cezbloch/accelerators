{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "037b4b81",
   "metadata": {},
   "source": [
    "# CUDA Environment Test\n",
    "\n",
    "This notebook executes environment test and uses **PyCUDA** to verify your CUDA toolkit + driver + compilers + Python stack are correctly installed.\n",
    "\n",
    "Run all cells from top to bottom. The final message should confirm success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf83588",
   "metadata": {},
   "source": [
    "## Imports and setup\n",
    "We import PyCUDA and NumPy. `pycuda.autoinit` creates a context automatically on the first CUDA device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3c22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit  # initializes CUDA context\n",
    "import pycuda.driver as drv\n",
    "from pycuda.compiler import SourceModule\n",
    "from cuda_helpers import profile_gpu\n",
    "\n",
    "print(f'CUDA Driver Version: {drv.get_driver_version()}')\n",
    "device = drv.Context.get_device()\n",
    "\n",
    "print(f'Using device: {device.name()} with compute capability {device.compute_capability()}')\n",
    "print(f'Total Memory (MB): {device.total_memory() / 1024**2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26730e33",
   "metadata": {},
   "source": [
    "## Host data\n",
    "We allocate large NumPy int32 arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb40610",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2**25  # 33,554,432 elements\n",
    "h_a = np.full(N, 1, dtype=np.int32)\n",
    "h_b = np.full(N, 2, dtype=np.int32)\n",
    "print(f'Working with {h_a.size:,} elements consuming {h_a.nbytes/1024**2:.2f} MB per array.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf45e19",
   "metadata": {},
   "source": [
    "## Device allocations and transfers\n",
    "Allocate device memory and transfer host arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ae4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_a = drv.mem_alloc(h_a.nbytes)\n",
    "d_b = drv.mem_alloc(h_b.nbytes)\n",
    "d_c = drv.mem_alloc(h_a.nbytes)\n",
    "drv.memcpy_htod(d_a, h_a)\n",
    "drv.memcpy_htod(d_b, h_b)\n",
    "print('Device buffers allocated & data transferred.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb62f6b",
   "metadata": {},
   "source": [
    "## CUDA kernel\n",
    "We reproduce the operation: c[i] = 2*a[i] + b[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_code = r'''\n",
    "        extern \"C\" __global__ void add_vectors(const int *a, const int *b, int *c, int N) {\n",
    "            int gid = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "            if (gid < N) {\n",
    "                c[gid] = 2 * a[gid] + b[gid];\n",
    "            }\n",
    "        }\n",
    "        '''\n",
    "mod = SourceModule(kernel_code, options=['-use_fast_math'])\n",
    "add_vectors = mod.get_function('add_vectors')\n",
    "print('Kernel compiled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55fc985",
   "metadata": {},
   "source": [
    "## Execution configuration\n",
    "Choose a block size (work-group size) and derive grid size (number of blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256  # threads per block\n",
    "grid_size = (N + block_size - 1) // block_size\n",
    "print(f'Launching with grid_size={grid_size}, block_size={block_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e16da9",
   "metadata": {},
   "source": [
    "## Run & profile kernel\n",
    "We wrap the kernel launch in a lambda for the profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1be7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch = lambda: add_vectors(d_a, d_b, d_c, np.int32(N), block=(block_size,1,1), grid=(grid_size,1))\n",
    "_ = profile_gpu(launch, n_warmup=2, n_iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b4dbbd",
   "metadata": {},
   "source": [
    "## Copy back & validate\n",
    "Transfer result back to host and compare with NumPy reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb7fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_c = np.empty_like(h_a)\n",
    "drv.memcpy_dtoh(h_c, d_c)\n",
    "expected = 2 * h_a + h_b\n",
    "np.testing.assert_array_equal(expected, h_c)\n",
    "print('Validation passed. If this message appears everything worked correctly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45970ea8",
   "metadata": {},
   "source": [
    "## Notes\n",
    "If any cell failed: check that the NVIDIA driver & CUDA toolkit are installed, and that `pycuda` is in your environment (see requirements or install via `pip install pycuda`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
