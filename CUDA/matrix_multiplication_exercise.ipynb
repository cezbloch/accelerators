{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8359a97b",
   "metadata": {},
   "source": [
    "# Matrix Multiplication with PyCUDA\n",
    "\n",
    "This notebook demonstrates how to perform matrix multiplication using PyCUDA. The CUDA kernel implementation is left empty for you to complete as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "from cuda_helpers import profile_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9101e",
   "metadata": {},
   "source": [
    "## Define Matrices\n",
    "\n",
    "Let's define two matrices to multiply. You can change their size and values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22163b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matrices A and B\n",
    "N = 1024\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15d57f",
   "metadata": {},
   "source": [
    "## Perform Matrix Multiplication on the GPU\n",
    "\n",
    "We will multiply matrices A and B using a CUDA kernel. Implement the kernel code to perform standard matrix multiplication as done i algebra. For two matrices A (of size m×n) and B (of size n×p), their product C = AB is an m×p matrix where each element C[i, j] is computed as the sum of products of the i-th row of A and the j-th column of B:\n",
    "\n",
    "$$\n",
    "C_{i,j} = \\sum_{k=1}^{N} A_{i,k} \\cdot B_{k,j}\n",
    "$$\n",
    "\n",
    "This operation is also known as \"GEMM\" (General Matrix Multiply) in numerical computing libraries.\n",
    "\n",
    "We will build-up the optimal solution in stages. Start simple, and implement the mutliplication using just global memory, where each thread computes output value for one cell in the output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate GPU memory and transfer matrices\n",
    "d_a = cuda.mem_alloc(A.nbytes)\n",
    "d_b = cuda.mem_alloc(B.nbytes)\n",
    "d_c = cuda.mem_alloc(A.nbytes)\n",
    "cuda.memcpy_htod(d_a, A)\n",
    "cuda.memcpy_htod(d_b, B)\n",
    "\n",
    "# CUDA kernel for matrix multiplication (to be completed)\n",
    "kernel_code = '''\n",
    "__global__ void matmul(float *A, float *B, float *C, int N) {\n",
    "    // TODO: Implement matrix multiplication kernel\n",
    "    // blockIdx, blockDim, threadIdx, gridDim\n",
    "    float sum = 0;\n",
    "    int2 global_id = make_int2(blockIdx.x * blockDim.x + threadIdx.x,\n",
    "                               blockIdx.y * blockDim.y + threadIdx.y);\n",
    "\n",
    "    if (global_id.x >= N || global_id.y >= N) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        int aij = global_id.y * N + i;\n",
    "        int bij = i * N + global_id.x;\n",
    "        sum  += A[aij] * B[bij];\n",
    "    }\n",
    "\n",
    "    int cij = global_id.y * N + global_id.x;\n",
    "    C[cij] = sum;\n",
    "}\n",
    "'''\n",
    "\n",
    "mod = SourceModule(kernel_code)\n",
    "matmul = mod.get_function(\"matmul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = (8, 8, 1)\n",
    "grid_size = (A.shape[0] // block_size[0], A.shape[1] // block_size[1], 1)\n",
    "\n",
    "print(f'Launching with grid_size={grid_size}, block_size={block_size}')\n",
    "\n",
    "n_warmup = 2\n",
    "n_iters = 50\n",
    "\n",
    "launch = lambda: matmul(d_a, d_b, d_c, np.int32(N), block=block_size, grid=grid_size)\n",
    "_ = profile_gpu(launch, n_warmup=n_warmup, n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6236bc",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "After running the kernel, copy the result back to the host and display it.\n",
    "\n",
    "Refer to the [solution](matrix_multiplication_solution_global.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result from GPU and display\n",
    "C = np.empty_like(A)\n",
    "cuda.memcpy_dtoh(C, d_c)\n",
    "c_numpy = np.matmul(A, B)\n",
    "print(\"Result matrix C (A x B):\\n\", C)\n",
    "\n",
    "np.testing.assert_almost_equal(C, c_numpy, decimal=3)\n",
    "# Note: You need to implement the kernel for correct results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023cfd73",
   "metadata": {},
   "source": [
    "# Shared memory caching\n",
    "\n",
    "Your next task is to optimize the kernel execution time. One of the common practices in matrix multipication is to optimize memory accesses. Elements in each input matrices A and B are accessed N times by reaching to global memory. While we can expect some level of caching happening under the hood, we can also cache parts of A and B matrices in Shared Local Memory.\n",
    "\n",
    "## Optimization task\n",
    "\n",
    "Write an optimized kernel which will cache blocks from A anb B matrices in SLM:\n",
    "- for similicity assume constant block size of (8, 8)\n",
    "- make sure there are not data races - so synchronize SLM accesses\n",
    "\n",
    "Implement the following algorithm:\n",
    "- for each cache block\n",
    "    - load 8x8 blocks from global memory into declared SLMs\n",
    "        - the main difficulty lays in accessing global memory based while iterating over cached blocks\n",
    "    - produce partial matrix multipication sum from data cached in SLM and accumulate in local variable\n",
    "        - here you just need local ids\n",
    "- dump accumulated sum into global memory C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d082bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_matrix_multiply(A, B, kernel_code):\n",
    "    N = A.shape[0]\n",
    "    assert A.shape[1] == N and B.shape[0] == N and B.shape[1] == N, \"Matrices must be square and same size\"\n",
    "\n",
    "    d_a = cuda.mem_alloc(A.nbytes)\n",
    "    d_b = cuda.mem_alloc(B.nbytes)\n",
    "    d_c = cuda.mem_alloc(A.nbytes)\n",
    "\n",
    "    cuda.memcpy_htod(d_a, A)\n",
    "    cuda.memcpy_htod(d_b, B)\n",
    "\n",
    "    mod = SourceModule(kernel_code)\n",
    "    matmul = mod.get_function(\"matmul\")\n",
    "\n",
    "    block_size = (8, 8, 1)\n",
    "    nr_threads = int(np.prod(block_size))\n",
    "    grid_size = (N // block_size[0], N // block_size[1], 1)\n",
    "    shared_mem_bytes = int(nr_threads * 4)\n",
    "    d_debug = cuda.mem_alloc(shared_mem_bytes)\n",
    "    \n",
    "    launch = lambda: matmul(d_a, d_b, d_c, d_debug,\n",
    "                            np.int32(N), block=block_size, grid=grid_size) #shared=shared_mem_bytes\n",
    "    launch()\n",
    "\n",
    "    C = np.empty_like(A)\n",
    "    cuda.memcpy_dtoh(C, d_c)\n",
    "    debug = np.zeros(nr_threads, dtype=np.int32)\n",
    "    cuda.memcpy_dtoh(debug, d_debug)\n",
    "    \n",
    "    c_numpy = np.matmul(A, B)\n",
    "    #print(\"Result matrix C (A x B):\\n\", C[:8, :8])\n",
    "    print(\"debug = \", debug[:8])\n",
    "\n",
    "    np.testing.assert_almost_equal(C, c_numpy, decimal=3)\n",
    "    \n",
    "    n_warmup = 2\n",
    "    n_iters = 100\n",
    "    \n",
    "    _ = profile_gpu(launch, n_warmup=n_warmup, n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_mem_kernel = '''\n",
    "    #define THREAD_INDEX (threadIdx.y * blockDim.x + threadIdx.x)\n",
    "    #define BLOCK_LENGTH 8 * 8\n",
    "\n",
    "    __global__ void matmul(float *A, float *B, float *C, int *debug, int N) {\n",
    "        float sum = 0;\n",
    "        int2 global_id = make_int2(blockIdx.x * blockDim.x + threadIdx.x,\n",
    "                                   blockIdx.y * blockDim.y + threadIdx.y);\n",
    "                                   \n",
    "        if (global_id.x >= N || global_id.y >= N) {\n",
    "            return;\n",
    "        }\n",
    "                \n",
    "        //extern __shared__ float slm[];\n",
    "        __shared__ float slm_A[BLOCK_LENGTH];\n",
    "        __shared__ float slm_B[BLOCK_LENGTH];\n",
    "        \n",
    "        for (int b = 0; b < gridDim.x; b++) {\n",
    "            int2 gidA = make_int2(b * blockDim.x + threadIdx.x,  blockIdx.y * blockDim.y + threadIdx.y);\n",
    "            int2 gidB = make_int2(blockIdx.x * blockDim.x + threadIdx.x,  b * blockDim.y + threadIdx.y);\n",
    "                        \n",
    "            slm_A[THREAD_INDEX] = A[gidA.y * N + gidA.x];\n",
    "            slm_B[THREAD_INDEX] = B[gidB.y * N + gidB.x];\n",
    "                \n",
    "            __syncthreads();\n",
    "            \n",
    "            for (int i = 0; i < blockDim.x; i++) {\n",
    "                sum += slm_A[blockDim.x * threadIdx.y + i] * slm_B[blockDim.x * i + threadIdx.x];\n",
    "            }            \n",
    "            \n",
    "            __syncthreads();\n",
    "        }\n",
    "        \n",
    "        C[global_id.y * N + global_id.x] = sum;\n",
    "    }\n",
    "'''\n",
    "    \n",
    "gpu_matrix_multiply(A, B, shared_mem_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025f528",
   "metadata": {},
   "source": [
    "Refer to the [solution](matrix_multiplication_solution_slm.cu) if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
