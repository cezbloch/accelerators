{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8359a97b",
   "metadata": {},
   "source": [
    "# Matrix Multiplication with PyCUDA\n",
    "\n",
    "This notebook demonstrates how to perform matrix multiplication using PyCUDA. The CUDA kernel implementation is left empty for you to complete as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd2c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule\n",
    "\n",
    "from cuda_helpers import profile_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba9101e",
   "metadata": {},
   "source": [
    "## Define Matrices\n",
    "\n",
    "Let's define two matrices to multiply. You can change their size and values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22163b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matrices A and B\n",
    "N = 1024\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15d57f",
   "metadata": {},
   "source": [
    "## Perform Matrix Multiplication on the GPU\n",
    "\n",
    "We will multiply matrices A and B using a CUDA kernel. Implement the kernel code to perform standard matrix multiplication as done i algebra. For two matrices A (of size m×n) and B (of size n×p), their product C = AB is an m×p matrix where each element C[i, j] is computed as the sum of products of the i-th row of A and the j-th column of B:\n",
    "\n",
    "$$\n",
    "C_{i,j} = \\sum_{k=1}^{N} A_{i,k} \\cdot B_{k,j}\n",
    "$$\n",
    "\n",
    "This operation is also known as \"GEMM\" (General Matrix Multiply) in numerical computing libraries.\n",
    "\n",
    "We will build-up the optimal solution in stages. Start simple, and implement the mutliplication using just global memory, where each thread computes output value for one cell in the output matrix.\n",
    "\n",
    "You can assume that all matrices are square for now, if you find it more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e45f028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate GPU memory and transfer matrices\n",
    "d_a = cuda.mem_alloc(A.nbytes)\n",
    "d_b = cuda.mem_alloc(B.nbytes)\n",
    "d_c = cuda.mem_alloc(A.nbytes)\n",
    "cuda.memcpy_htod(d_a, A)\n",
    "cuda.memcpy_htod(d_b, B)\n",
    "\n",
    "# CUDA kernel for matrix multiplication (to be completed)\n",
    "kernel_code = '''\n",
    "__global__ void matmul(float *A, float *B, float *C, int N) {\n",
    "    // TODO: Implement matrix multiplication kernel\n",
    "    // blockIdx, blockDim, threadIdx, gridDim\n",
    "    float sum = 0;\n",
    "    int2 global_id = make_int2(blockIdx.x * blockDim.x + threadIdx.x,\n",
    "                               blockIdx.y * blockDim.y + threadIdx.y);\n",
    "\n",
    "    if (global_id.x >= N || global_id.y >= N) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // your code goes here\n",
    "}\n",
    "'''\n",
    "\n",
    "mod = SourceModule(kernel_code)\n",
    "matmul = mod.get_function(\"matmul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cbb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = (8, 8, 1)\n",
    "grid_size = (A.shape[0] // block_size[0], A.shape[1] // block_size[1], 1)\n",
    "\n",
    "print(f'Launching with grid_size={grid_size}, block_size={block_size}')\n",
    "\n",
    "n_warmup = 2\n",
    "n_iters = 50\n",
    "\n",
    "launch = lambda: matmul(d_a, d_b, d_c, np.int32(N), block=block_size, grid=grid_size)\n",
    "_ = profile_gpu(launch, n_warmup=n_warmup, n_iters=n_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6236bc",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "After running the kernel, copy the result back to the host and display it.\n",
    "\n",
    "Refer to the [solution](matrix_multiplication_solution_global.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result from GPU and display\n",
    "C = np.empty_like(A)\n",
    "cuda.memcpy_dtoh(C, d_c)\n",
    "c_numpy = np.matmul(A, B)\n",
    "print(\"Result matrix C (A x B):\\n\", C)\n",
    "\n",
    "np.testing.assert_almost_equal(C, c_numpy, decimal=3)\n",
    "# Note: You need to implement the kernel for correct results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023cfd73",
   "metadata": {},
   "source": [
    "# Shared memory caching\n",
    "\n",
    "Your next task is to optimize the kernel execution time. One of the common practices in matrix multipication is to optimize memory accesses. Elements in each input matrices A and B are accessed N times by reaching to global memory. While we can expect some level of caching happening under the hood, we can also cache parts of A and B matrices in Shared Local Memory.\n",
    "\n",
    "Below is a helper function that will launch your new kernel. It will:\n",
    "* validate the sized of your matrices\n",
    "* automatically allocate dynamic share memory for matrices A and B\n",
    "* transfer matrices from host to device and back\n",
    "* launch your kernel\n",
    "* verify correctness of the calculations against numpy implementation\n",
    "* if the results are correct it will launch the kernel multiple times to measure execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48942935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_matrix_multiply(A, B, kernel_code, block_size=(16, 16, 1), warmup=2, iters=100, *args, **kwargs):\n",
    "    import numpy as np\n",
    "    import pycuda.driver as cuda\n",
    "    from pycuda.compiler import SourceModule\n",
    "    from cuda_helpers import profile_gpu\n",
    "\n",
    "    M, N_A = A.shape\n",
    "    N_B, P = B.shape\n",
    "    print(f\"A: MxN ({M}, {N_A}), B: NxP ({N_B}, {P}), C: MxP ({M}, {P})\")\n",
    "    assert N_A == N_B, \"Inner matrix dimensions must match\"\n",
    "\n",
    "    d_a = cuda.mem_alloc(A.nbytes)\n",
    "    d_b = cuda.mem_alloc(B.nbytes)\n",
    "    d_c = cuda.mem_alloc(M * P * 4)\n",
    "    cuda.memcpy_htod(d_a, A)\n",
    "    cuda.memcpy_htod(d_b, B)\n",
    "\n",
    "    mod = SourceModule(kernel_code)\n",
    "    matmul = mod.get_function(\"matmul\")\n",
    "\n",
    "    bx, by, bz = block_size\n",
    "    grid_x = (P + bx - 1) // bx\n",
    "    grid_y = (M + by - 1) // by\n",
    "    grid_size = (grid_x, grid_y, 1)\n",
    "\n",
    "    use_slm = 'extern __shared__' in kernel_code\n",
    "    if use_slm:\n",
    "        nr_threads = bx * by\n",
    "        shared_mem_bytes = nr_threads * 4 * 2\n",
    "    else:\n",
    "        shared_mem_bytes = 0\n",
    "\n",
    "    def launch():\n",
    "        if shared_mem_bytes:\n",
    "            matmul(d_a, d_b, d_c, *args, block=block_size, grid=grid_size, shared=shared_mem_bytes, **kwargs)\n",
    "        else:\n",
    "            matmul(d_a, d_b, d_c, *args, block=block_size, grid=grid_size, **kwargs)\n",
    "            \n",
    "    print(f'Launching with grid_size={grid_size}, block_size={block_size}, shared_mem_bytes={shared_mem_bytes}')\n",
    "    launch()\n",
    "\n",
    "    C = np.empty((M, P), dtype=np.float32)\n",
    "    cuda.memcpy_dtoh(C, d_c)\n",
    "    ref = np.matmul(A, B)\n",
    "    np.testing.assert_almost_equal(C, ref, decimal=3)\n",
    "\n",
    "    _ = profile_gpu(launch, n_warmup=warmup, n_iters=iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a141ab",
   "metadata": {},
   "source": [
    "## Optimization task\n",
    "\n",
    "Write an optimized kernel which will cache blocks from A anb B matrices in SLM:\n",
    "- for similicity assume that the shapes of all matrices are multiple of block size\n",
    "- make sure there are not data races - so synchronize SLM accesses\n",
    "\n",
    "Implement the following algorithm:\n",
    "- for each cache block\n",
    "    - load 8x8 blocks from global memory into declared SLMs\n",
    "        - the main difficulty lays in accessing global memory based while iterating over cached blocks\n",
    "    - produce partial matrix multipication sum from data cached in SLM and accumulate in local variable\n",
    "        - here you just need local ids\n",
    "- dump accumulated sum into global memory C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c368fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_mem_kernel = '''\n",
    "    #define THREAD_INDEX (threadIdx.y * blockDim.x + threadIdx.x)\n",
    "\n",
    "    __global__ void matmul(float *A, float *B, float *C, int N) {\n",
    "        float sum = 0;\n",
    "        int2 global_id = make_int2(blockIdx.x * blockDim.x + threadIdx.x,\n",
    "                                   blockIdx.y * blockDim.y + threadIdx.y);\n",
    "                \n",
    "        extern __shared__ float slm[];\n",
    "        float *slm_A = &slm[0];\n",
    "        float *slm_B = &slm[blockDim.x * blockDim.y];\n",
    "        \n",
    "        // your code goes here\n",
    "    }\n",
    "'''\n",
    "\n",
    "# For square matrices, pass N as kernel argument\n",
    "args = (np.int32(N),)\n",
    "gpu_matrix_multiply(A, B, \n",
    "                    shared_mem_kernel,\n",
    "                    (16, 16, 1),\n",
    "                    2, 100,\n",
    "                    *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b6ccb",
   "metadata": {},
   "source": [
    "Refer to the [solution](matrix_multiplication_solution_slm.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080f28d",
   "metadata": {},
   "source": [
    "# Non-square matrices\n",
    "\n",
    "Adapt your solution to work with non-square matrices. You will to:\n",
    "* pass sizes of matrices to your kernel - as they are non-square so can have different shapes than only N.\n",
    "* Use this shapes in your kernel to access elements.\n",
    "* think about iterating through consecutive cached blocks in the for-loop. Make sure to look iterate from the perspective of output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_square_kernel = '''\n",
    "    #define THREAD_INDEX (threadIdx.y * blockDim.x + threadIdx.x)\n",
    "\n",
    "    __global__ void matmul(float *A, float *B, float *C, int M, int N, int P) {\n",
    "        float sum = 0;\n",
    "        int2 global_id = make_int2(blockIdx.x * blockDim.x + threadIdx.x,\n",
    "                                   blockIdx.y * blockDim.y + threadIdx.y);\n",
    "\n",
    "        extern __shared__ float slm[];\n",
    "        float *slm_A = &slm[0];\n",
    "        float *slm_B = &slm[blockDim.x * blockDim.y];\n",
    "        \n",
    "        int nr_blocks = (N + blockDim.x - 1) / blockDim.x;\n",
    "\n",
    "        // your code goes here\n",
    "    }\n",
    "'''\n",
    "\n",
    "A_ = np.random.randn(512, 1024).astype(np.float32)\n",
    "B_ = np.random.randn(1024, 768).astype(np.float32)\n",
    "# For non-square matrices, pass M, N, P as kernel arguments\n",
    "M, N, P = A_.shape[0], A_.shape[1], B_.shape[1]\n",
    "args = (np.int32(M), np.int32(N), np.int32(P))\n",
    "gpu_matrix_multiply(A_, B_, \n",
    "                    non_square_kernel,\n",
    "                    (16, 16, 1),\n",
    "                    2, 100,\n",
    "                    *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4025f528",
   "metadata": {},
   "source": [
    "Refer to the [solution](matrix_multiplication_solution_slm_non_square.cu) if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
