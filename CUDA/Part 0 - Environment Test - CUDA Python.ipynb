{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e62224",
   "metadata": {},
   "source": [
    "# CUDA Python (Numba) Environment Test\n",
    "\n",
    "This notebook replicates the original PyOpenCL environment test using **CUDA Python via Numba**. It validates that your NVIDIA driver + CUDA toolkit and the Numba CUDA runtime are working.\n",
    "\n",
    "Run all cells in order. The final message should confirm success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f7dabc",
   "metadata": {},
   "source": [
    "## 1. Imports and device info\n",
    "We query available CUDA devices through Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096519bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'NVIDIA GeForce RTX 2060 with Max-Q Design'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-42a52008-a233-7f47-7c29-d01344a0b937\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "Active device: NVIDIA GeForce RTX 2060 with Max-Q Design (CC (7, 5))\n",
      "Max threads per block: 1024\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "import numpy as np\n",
    "\n",
    "cuda.detect()  # prints detected devices\n",
    "device = cuda.get_current_device()\n",
    "print(f'Active device: {device.name.decode()} (CC {device.compute_capability})')\n",
    "print(f'Max threads per block: {device.MAX_THREADS_PER_BLOCK}')\n",
    "#print(f'Memory (bytes): {device.total_memory}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac894b1",
   "metadata": {},
   "source": [
    "## 2. Profiling helper\n",
    "We implement a timing helper using CUDA events through Numba's driver APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26782970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.cuda.cudadrv import driver as _drv\n",
    "\n",
    "def profile_gpu(fn, n_warmup, n_iters, *launch_args):\n",
    "    # warmup\n",
    "    for _ in range(n_warmup):\n",
    "        fn(*launch_args)\n",
    "        cuda.synchronize()\n",
    "    times = np.zeros(n_iters, dtype=np.float64)\n",
    "    for i in range(n_iters):\n",
    "        start = _drv.event()\n",
    "        end = _drv.event()\n",
    "        start.record()\n",
    "        fn(*launch_args)\n",
    "        end.record()\n",
    "        end.synchronize()\n",
    "        times[i] = _drv.event_elapsed_time(start, end)  # ms\n",
    "    print(f'Kernel average {times.mean():.4f} ms, median {np.median(times):.4f} ms, std {times.std():.4f} ms over {n_iters} runs.')\n",
    "    return times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae4af0",
   "metadata": {},
   "source": [
    "## 3. Host data\n",
    "Allocate large host arrays (int32) like in the OpenCL example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1588c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 33,554,432 elements; each array uses 128.00 MB.\n"
     ]
    }
   ],
   "source": [
    "N = 2**25  # 33,554,432\n",
    "h_a = np.full(N, 1, dtype=np.int32)\n",
    "h_b = np.full(N, 2, dtype=np.int32)\n",
    "print(f'Working with {h_a.size:,} elements; each array uses {h_a.nbytes/1024**2:.2f} MB.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa49575",
   "metadata": {},
   "source": [
    "## 4. Device arrays\n",
    "Transfer host arrays to device using Numba's `to_device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e4f39b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m d_a = \u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh_a\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m d_b = cuda.to_device(h_b)\n\u001b[32m      3\u001b[39m d_c = cuda.device_array_like(h_a)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\accelerators\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:231\u001b[39m, in \u001b[36mrequire_context.<locals>._require_cuda_context\u001b[39m\u001b[34m(*args, **kws)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(fn)\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_require_cuda_context\u001b[39m(*args, **kws):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_runtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mensure_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkws\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python\\Python311\\Lib\\contextlib.py:137\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\accelerators\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:123\u001b[39m, in \u001b[36m_Runtime.ensure_context\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m driver.get_active_context():\n\u001b[32m    122\u001b[39m     oldctx = \u001b[38;5;28mself\u001b[39m._get_attached_context()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     newctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_attached_context(newctx)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\accelerators\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:138\u001b[39m, in \u001b[36m_Runtime.get_or_create_context\u001b[39m\u001b[34m(self, devnum)\u001b[39m\n\u001b[32m    136\u001b[39m attached_ctx = \u001b[38;5;28mself\u001b[39m._get_attached_context()\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attached_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_or_create_context_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attached_ctx\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\accelerators\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:158\u001b[39m, in \u001b[36m_Runtime._get_or_create_context_uncached\u001b[39m\u001b[34m(self, devnum)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._activate_context_for(\u001b[32m0\u001b[39m)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# Get primary context for the active device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     ctx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mac\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m]\u001b[49m.get_primary_context()\n\u001b[32m    159\u001b[39m     \u001b[38;5;66;03m# Is active context the primary context?\u001b[39;00m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m USE_NV_BINDING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\projects\\accelerators\\.venv\\Lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:40\u001b[39m, in \u001b[36m_DeviceList.__getitem__\u001b[39m\u001b[34m(self, devnum)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    Returns the context manager for device *devnum*.\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    '''\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "d_a = cuda.to_device(h_a)\n",
    "d_b = cuda.to_device(h_b)\n",
    "d_c = cuda.device_array_like(h_a)\n",
    "print('Device arrays created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6275072",
   "metadata": {},
   "source": [
    "## 5. CUDA kernel (Numba)\n",
    "Define the kernel performing c[i] = 2*a[i] + b[i]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f811511",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_vectors(a, b, c):\n",
    "    gid = cuda.grid(1)\n",
    "    if gid < a.size:\n",
    "        c[gid] = 2 * a[gid] + b[gid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f4cd9",
   "metadata": {},
   "source": [
    "## 6. Execution configuration\n",
    "Define threads per block & blocks per grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28225c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_per_block = 256\n",
    "blocks_per_grid = (N + threads_per_block - 1) // threads_per_block\n",
    "print(f'blocks={blocks_per_grid}, threads_per_block={threads_per_block}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7771d541",
   "metadata": {},
   "source": [
    "## 7. Run & profile kernel\n",
    "Wrap the launch in a lambda for profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5a7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "launch = lambda: add_vectors[blocks_per_grid, threads_per_block](d_a, d_b, d_c)\n",
    "_ = profile_gpu(launch, n_warmup=2, n_iters=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e5fbe",
   "metadata": {},
   "source": [
    "## 8. Copy back & validate\n",
    "Copy result and compare with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_c = d_c.copy_to_host()\n",
    "expected = 2 * h_a + h_b\n",
    "np.testing.assert_array_equal(expected, h_c)\n",
    "print('Validation passed. If this message appears everything worked correctly.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53703e",
   "metadata": {},
   "source": [
    "## 9. Notes\n",
    "If detection fails: ensure an NVIDIA GPU, driver, CUDA toolkit, and that `numba` detects the CUDA toolkit. Install with `pip install numba`. Optionally remove PyCUDA if not needed anymore."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
