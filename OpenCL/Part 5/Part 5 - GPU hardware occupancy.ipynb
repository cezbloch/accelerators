{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy calculator\n",
    "\n",
    "Maximizing perfomance of compute kernel is a complex task depending on many factors. It's not uncommon that individual code changes don't yield improvements but combined together they perform faster. Going blindly with trial and error is a way to get somewhere but it's wiser to optimize your code with some knowledge of underlying GPU architecture.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "Each of the GPU vendors produces it's own devices in a different way - there are different component, memory or registers can be located on different levels.\n",
    "\n",
    "Frameworks like OpenCL distinguish some of the building blocks while leaving others up to vendors, so are unnamed. We will follow Khronos OpenCL naming. Below is the high level list from highest (work groups level) to lowest level (close to the thread):\n",
    "* Compute Device - mostly it's GPU - divides work groups to Compute Units (CU)\n",
    "* Compute Unit (CU) - consists of multiple PBs processes entire work groups, divides work group into subgroups and sends them lower. On major platforms it has SLM. On some it has register files as well.\n",
    "* Processing Block (PB)- works on a subgroup level, has ALUs (Arightmetic Logic Units) and FPU (Floating Point Units ) and usually has registers to keep the state of kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSbabHdA1t-2f9uM6TsUtn4eiUITPP_lZljBa8cCGgEDz-XLzQUtleg5b-_O0AK9jiN_s0ecx-oqiGw/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are vendor specific building block names.\n",
    "\n",
    "Nvidia (Turing):\n",
    "* CUDA Core or Streaming Processor - processes onekernel instance, has an ALU and FPU\n",
    "* Processing Block (PB) - works with subgroups, has 16kB register file, has warp scheduler, L0 cache\n",
    "* Streaming Multiprocessor (SM) - wraps 4 PBs, works with entire work groups - has SLM and L1 cache\n",
    "* Texture Processing Cluster (TPC) - wraps one SM\n",
    "* Graphics Processing Clusters (GPC) - wraps several TPCs, one or more of them create GPU\n",
    "* GPU - consists of one or more GPCs, has L2 cache\n",
    "\n",
    "Intel (Gen 11):\n",
    "* 2 ALUs - together processing 8 threads\n",
    "* Execution Unit (EU) - The foundational building block - has register file, processes several subgroups, 2 ALUs where actual calcualtions happen\n",
    "* Subslice - works on entire work groups - has SLM, can distribute work group to EUs\n",
    "* Slice - can dispatch entier work groups to different subslices, has L3 cache. If there are no barriers in the code and work items are independent within work groups, work groups can be scheduled to multiple subslices\n",
    "* GPU - consist of one or more Slices\n",
    "\n",
    "Mapping of those building blocks to generalized architexture is documented on [GPU Terminology Dictionary](https://confluence.synaptics.com/display/IOTVI/GPU+Terminology+Dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSKOdrKeIO8oPMlm1f1ypEJcdWx3BvY4zntgfj0fABYdllJaXGpjJEq1reFeo4-tfEyAKD_pQjogLGG/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Unit Occupancy\n",
    "\n",
    "One of the things we can calculate statically - without running the shader - is subgroup occupancy. It can tell us how well our compiled shader theoretically utilizes CU.\n",
    "\n",
    "To calculate it we need some information about the CU:\n",
    "* SLM size in bytes\n",
    "* registers file size in PB per subgroup\n",
    "* register file size per CU\n",
    "* number of subgroup states - how many subgroups can be kept in register file. When GPU is processing a subgroup, other subgroup state is kept in register space, those other subgroups can wait for eg. memory transactions or for other subgroups to finish their work.\n",
    "* number of processing blocks\n",
    "* maximum number of work groups which can be active at the particlar moment\n",
    "\n",
    "And some information about the kernel binary compiled for that particular platform:\n",
    "* number of threads per work group - it's part of your execution configuration - local work group size\n",
    "* SLM usage per work group - easy to calculate - from the sizes of your local memory arrays, it's also possible to query OpenCL API on the program object.\n",
    "\n",
    "* subgroup size - can be deduced from \"preferred work group size multiple\" returned by OpenCL API call.\n",
    "* registers usage per kernel instance - depend on a platform's compiler:\n",
    " - NVidia - specifying compiler option to get the value\n",
    " - Adreno - by querying private bytes through OpenCL API.\n",
    " - Mali - can only return spilled registers amount when queried for private bytes.\n",
    " - Intel - we don't have a reliable way to get that information. A hint of how many registers are used could be the DirectCompute HLSL bytecode value - dcl_temps - this value should be multiplied by 16 - to figure out why by 16 have a look \n",
    "[here](https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dcl-temps)\n",
    " \n",
    " \n",
    "### Calculating CU occupancy\n",
    " \n",
    "Below there are values filled for some platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Nvidia RTX 2080 Ti ------\n",
    "\n",
    "# Processing block\n",
    "processing_block = {\n",
    "    'register_file_in_bytes': 8192 * 4, # 32 threads * 255 max registers * 4 - in the docs it's 16384 * 4 per PB\n",
    "    'nr_subgroups_states': 16, # this number is calculated, \n",
    "    # Turing has a limit of 2048 threads per SM. Divided by subgroup width of 32 and by 4 PBs gives - 2048/(32 * 4) = 16\n",
    "}\n",
    "\n",
    "# Streaming Multiprocessor\n",
    "compute_unit = {\n",
    "    'slm_size_in_bytes': 65536,\n",
    "    'register_file_in_bytes': 4 * 65536, # from documentation 65536 32 bit\n",
    "    'max_active_work_groups': 16,\n",
    "    'nr_processing_blocks': 4\n",
    "}\n",
    "\n",
    "nvidia_rtx_2080_ti_platform = {\n",
    "    'name' : \"NVidia RTX 2080 Ti\",\n",
    "    'compute_unit' : compute_unit,\n",
    "    'processing_block': processing_block\n",
    "}\n",
    "\n",
    "# ------ Intel Gen 11 ------\n",
    "\n",
    "# Execution Unit\n",
    "processing_block = {\n",
    "    'register_file_in_bytes': 4096, # this is a register limit per subgroup\n",
    "    'nr_subgroups_states': 7 # this actually means how many subgroup states can be kept in hardware\n",
    "}\n",
    "\n",
    "# Subslice\n",
    "compute_unit = {\n",
    "    'slm_size_in_bytes': 65536,\n",
    "    'register_file_in_bytes': 229376, # this is (subgroup register file) x (nr subgroup states) x (nr of processing blocks)\n",
    "    'max_active_work_groups': 16,\n",
    "    'nr_processing_blocks': 8\n",
    "}\n",
    "\n",
    "intel_gen_11_platform = {\n",
    "    'name' : \"Intel Gen 11\",    \n",
    "    'compute_unit' : compute_unit,\n",
    "    'processing_block': processing_block\n",
    "}\n",
    "\n",
    "# ------ ARM Mali G52 ------\n",
    "\n",
    "# Execution Engine\n",
    "processing_block = {\n",
    "    'register_file_in_bytes': 8 * 64 * 4, # each thread can store 64 32 bits wide register - 8 threads per subgroup\n",
    "    'nr_subgroups_states': 16 # deduced from the limit of work groups size - 384 - can be wrong\n",
    "}\n",
    "\n",
    "# Core\n",
    "compute_unit = {\n",
    "    'slm_size_in_bytes': 32 * 1024, # returned by OpenCL - SLM on Mali is part of global memory\n",
    "    'register_file_in_bytes': 384 * 64 * 4, # 96kB\n",
    "    'max_active_work_groups': 16, # this is a guess based on nothing - taken from intel - may be wrong\n",
    "    'nr_processing_blocks': 3\n",
    "}\n",
    "\n",
    "mali_g52_platform = {\n",
    "    'name' : \"Mali G52 MP2\",\n",
    "    'compute_unit' : compute_unit,\n",
    "    'processing_block': processing_block\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are 3 main functions calculating the subgroup limit with respect to particular resource:\n",
    "* register usage\n",
    "    - check if registers are spilled by calculating if subgroup fits into it's register limits\n",
    "    - calculate how many work group states fit into total CU register space\n",
    "* SLM\n",
    "* subgroups\n",
    "    - calculate how many subgroup states (from entire work groups) can be stored on a CU\n",
    "* hardware design limits defined by vendor in documentation\n",
    "    - maximum number of work groups which can be active - determined by other factors\n",
    "\n",
    "From these limits we take the minimum - because it's the bottleneck. Based on that, we can calculate how many subgroups can be scheduled. Then we compare it against maximum number of possible subgroups we could have. This ratio gives the occupancy percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#occupancy calculator\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def calculate_register_limits(compute_unit, processing_block, kernel):\n",
    "    # Calculate maximum number of registers available per thread\n",
    "    available_regs_per_thread = ceil(processing_block['register_file_in_bytes'] / kernel['subgroup_size'])    \n",
    "    \n",
    "    # Check if registers are spilled\n",
    "    if kernel['register_usage_in_bytes'] > available_regs_per_thread:\n",
    "        return 0\n",
    "    \n",
    "    reg_per_compute_unit = compute_unit['register_file_in_bytes']\n",
    "    # Calculate register usage per work group\n",
    "    reg_per_wk = kernel['register_usage_in_bytes'] * kernel['work_group_size']\n",
    "    \n",
    "    # Calculate how many work group states fit into total CU register space    \n",
    "    return reg_per_compute_unit//reg_per_wk\n",
    "\n",
    "\n",
    "def calculate_slm_limits(compute_unit, kernel):    \n",
    "    slm_per_compute_unit = compute_unit['slm_size_in_bytes']\n",
    "    slm_per_wk = kernel['slm_usage_per_work_group_in_bytes']\n",
    "    \n",
    "    # Calculate how many work groups fit into total CU SLM space\n",
    "    return slm_per_compute_unit//slm_per_wk\n",
    "\n",
    "\n",
    "def calculate_subgroup_limits(compute_unit, processing_block, kernel):\n",
    "    # Calculate how many subgroup states can be stored on a CU, given work groups cannot be split across CUs\n",
    "    \n",
    "    # Total number of subgroups that can be stored on a CU\n",
    "    total_subgroups = compute_unit['nr_processing_blocks'] * processing_block['nr_subgroups_states']\n",
    "    # Number of subgroups per work group\n",
    "    subgroups_per_work_group = ceil(kernel['work_group_size'] / kernel['subgroup_size'])\n",
    "    \n",
    "    # Calculate how many work groups fit into total CU subgroup states\n",
    "    return total_subgroups//subgroups_per_work_group\n",
    "\n",
    "\n",
    "def describe_compute_unit(compute_unit):\n",
    "    print(\"Compute Unit\")\n",
    "    print(f\"\\tShared Memory Size (bytes): {compute_unit['slm_size_in_bytes']} \")\n",
    "    print(f\"\\tRegister File (bytes): {compute_unit['register_file_in_bytes']} \")\n",
    "    print(f\"\\tMaximum number of Active Work Groups: {compute_unit['max_active_work_groups']} \")\n",
    "    print(f\"\\tNumber of Processing Blocks: {compute_unit['nr_processing_blocks']} \")\n",
    "\n",
    "    \n",
    "def describe_processing_block(processing_block):\n",
    "    print(\"Processing Block\")\n",
    "    print(f\"\\tNumber of subgroup states: {processing_block['nr_subgroups_states']} \")\n",
    "    print(f\"\\tRegister File per subgroup (bytes): {processing_block['register_file_in_bytes']} \")\n",
    "\n",
    "\n",
    "def describe_kernel(kernel):\n",
    "    print(f\"Kernel - {kernel['name']}\")\n",
    "    print(f\"\\tWork Group Size (number of threads in Work Group): {kernel['work_group_size']} \")       \n",
    "    print(f\"\\tSubgroup size: {kernel['subgroup_size']} \")          \n",
    "    print(f\"\\tLocal Shared Memory usage per Work Group (bytes): {kernel['slm_usage_per_work_group_in_bytes']} \")\n",
    "    print(f\"\\tRegister usage per kernel (bytes): {kernel['register_usage_in_bytes']} \")\n",
    "\n",
    "          \n",
    "def occupancy(platform, kernel):\n",
    "    compute_unit = platform['compute_unit']\n",
    "    processing_block = platform['processing_block']\n",
    "    \n",
    "    reg_limit = calculate_register_limits(compute_unit, processing_block, kernel)\n",
    "    slm_limit = calculate_slm_limits(compute_unit, kernel)\n",
    "    subgroup_limit = calculate_subgroup_limits(compute_unit, processing_block, kernel)\n",
    "\n",
    "    max_wks = compute_unit['max_active_work_groups']\n",
    "    \n",
    "    limit = min(reg_limit, slm_limit, subgroup_limit, max_wks)\n",
    "    \n",
    "    print(f\"-------------------------------------------------------------------------------------\\n\")\n",
    "    print(f\"Platform: {platform['name']}\")\n",
    "    describe_compute_unit(compute_unit)\n",
    "    describe_processing_block(processing_block)\n",
    "    describe_kernel(kernel)\n",
    "\n",
    "    print(\"\")\n",
    "    print(f\"{limit} work groups is the final limit.\")\n",
    "    \n",
    "    print(f\"\\t{max_wks} work groups limit imposed by hardware design.\")\n",
    "    \n",
    "    if reg_limit > 0:\n",
    "        print(f\"\\t{reg_limit} work groups limit imposed by register usage of your kernel.\")\n",
    "    else:\n",
    "        print(f\"\\tYou kernel uses too many registers and it's not possible to schedule any work.\")\n",
    "        \n",
    "    if subgroup_limit > 0:\n",
    "        print(f\"\\t{subgroup_limit} work groups limit imposed by your work group size and compiled subgroup width.\")\n",
    "    else:\n",
    "        print(f\"\\tYou kernel uses too many registers and it's not possible to schedule any work.\")\n",
    "        \n",
    "    if slm_limit > 0:\n",
    "        print(f\"\\t{slm_limit} work groups limit imposed by SLM usage of your kernel.\")\n",
    "    else:\n",
    "        print(f\"\\tYou kernel uses too much SLM and it's not possible to schedule any work.\")\n",
    "    \n",
    "    threads_per_wk = kernel['work_group_size']\n",
    "    subgroup_size = kernel['subgroup_size']\n",
    "    \n",
    "    # Number of subgroups per work group\n",
    "    subgroups_per_wk = ceil(threads_per_wk / subgroup_size)\n",
    "    # Number of active subgroups per compute unit\n",
    "    subgroups_per_compute_unit = subgroups_per_wk * limit\n",
    "    \n",
    "    # Total number of subgroups that could be stored on a CU\n",
    "    total_subgroups = compute_unit['nr_processing_blocks'] * processing_block['nr_subgroups_states']\n",
    "    \n",
    "    # ------------- Compute Unit OCCUPANCY -------------------------\n",
    "    # Tells how many of the subgroups can be stored and processed on a CU\n",
    "    # this can get below 100% if your kernel uses too many registers or SLM\n",
    "    subgroup_occupancy = 100 * subgroups_per_compute_unit / total_subgroups\n",
    "    \n",
    "    print(f\"{subgroup_occupancy:.2f}% subgroup occupany of Compute Unit.\")\n",
    "    print(f\"\\t{subgroups_per_wk} subgroups per work group.\")\n",
    "    print(f\"\\t{subgroups_per_compute_unit} of {total_subgroups} available subgroups are utilized by your kernel.\")    \n",
    "    \n",
    "    # Thread occupancy - tells how many lanes (or cores) in your subgroup are utilized\n",
    "    # this can get below 100% only if your work group size is not multiple of subgroup width\n",
    "    # this is independent of register or SLM usage\n",
    "    # Number of threads per work group ceiled to the next multiple of subgroup size\n",
    "    threads_per_wk_ceiled = subgroups_per_wk * subgroup_size\n",
    "    thread_occupancy = 100 * threads_per_wk / threads_per_wk_ceiled\n",
    "    print(f\"{thread_occupancy:.2f}% thread occupany of your subgroups.\")\n",
    "    print(f\"\\t{subgroup_size} threads per subgroup.\")\n",
    "    print(f\"\\t{threads_per_wk} of {threads_per_wk_ceiled} running threads are active in your kernel.\")        \n",
    "    print(f\"-------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define kernel specific information and feed it to the occupancy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernel_a = {\n",
    "    'name' : 'Kernel A on Intel Gen 11',\n",
    "    'work_group_size' : 48, \n",
    "    'slm_usage_per_work_group_in_bytes' : 768,\n",
    "    'subgroup_size' : 8,\n",
    "    'register_usage_in_bytes': 384\n",
    "}\n",
    "\n",
    "kernel_b = {\n",
    "    'name' : 'Kernel B on Intel Gen 11',\n",
    "    'work_group_size' : 64,\n",
    "    'slm_usage_per_work_group_in_bytes' : 384,\n",
    "    'subgroup_size' : 16,\n",
    "    'register_usage_in_bytes': 256\n",
    "}\n",
    "\n",
    "occupancy(intel_gen_11_platform, kernel_a)\n",
    "\n",
    "kernel_nvidia = {\n",
    "    'name' : 'Kernel B compiled on NVidia',\n",
    "    'work_group_size' : 128,\n",
    "    'slm_usage_per_work_group_in_bytes' : 384,\n",
    "    'subgroup_size' : 32,\n",
    "    'register_usage_in_bytes': 300\n",
    "}\n",
    "\n",
    "occupancy(nvidia_rtx_2080_ti_platform, kernel_nvidia)\n",
    "\n",
    "kernel_mali = {\n",
    "    'name' : 'Soft kernel on Mali',\n",
    "    'work_group_size' : 32, \n",
    "    'slm_usage_per_work_group_in_bytes' : 768,\n",
    "    'subgroup_size' : 8,\n",
    "    'register_usage_in_bytes': 64 * 4\n",
    "}\n",
    "\n",
    "occupancy(mali_g52_platform, kernel_mali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those theoretical occupancy results are a guideline when designing your kernels. Better performance is not always linked to high occupancy as it depends on other factors as well. Your kernel may perform computations inefficiently, access memory in non optimal fashion or have lot of divergent paths. However low occupancy may give you a hint to change something when your kernels are memory bound. With higher occupancy you may hide more memory latency. \n",
    "\n",
    "The calculations here are a simplification. For example, on NVidia platforms warps are allocated in groups which was neglected here.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Play around with Nvidia kernel B configuration to get 100% subgroup occupancy.\n",
    "* you are on NVidia platform so you cannot change the subgroup size - it has to remain 32\n",
    "* assume you do imaginary code changes so you can manipulate all other kernel parameters\n",
    "* change the values in code examples above\n",
    "\n",
    "Refer to the [solution](./occupancy_solution.py) if you get stuck.\n",
    "\n",
    "## Compute Device Occupancy\n",
    "\n",
    "Keeping your entire GPU busy is more straightforward. We should have at least as many work-groups as Compute Units. If you create less Work Groups then some Compute Units will remain idle.\n",
    "\n",
    "For example assume we have 5 Compute Units. You have optimized your Compute Unit occupancy and it will fit 6 work groups. When it would make sense to create multiple of number of work group that fit on CU times number of CUs.\n",
    "In this example it will give 5 CUs * 6 work groups on each CU which gives 30 work groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other performance hints\n",
    "\n",
    "#### Static arrays in kernel code\n",
    "\n",
    "On various platforms statically declared arrays (like 'int my_array[64]') turned out to cause performance issues.\n",
    "\n",
    "On Adreno it seems like they are treated as global memory access and are slowing down kernel performance a **couple of times**.\n",
    "\n",
    "Often registers can be reused. So you can have for example 50 variables but they may require only 25 registers - because some or the variables go out of scope of will not be needed at a further place in your kernel. But if you put all those variables in an array instead of separate variables, the compiler will not optimize the size of the array but will keep it at the declared size.\n",
    "\n",
    "#### Number of memory barriers\n",
    "\n",
    "There is a dedicated hardware handling barriers with limited capacity. If this capacity is exceeded performance drops. \n",
    "This topic should be studies more in depth but it's good to be aware of such limitation.\n",
    "\n",
    "Using barriers in your code can cause your subgroup size to drop - observed on Adreno."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
