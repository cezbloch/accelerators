{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Local Memory - SLM\n",
    "\n",
    "This notebook introduces a mechanism that enables data exchange between threads in GPU. By using Shared Local Memory (SLM), often located close to the computing cores, it is possible to do it efficiently. \n",
    "\n",
    "## Setup\n",
    "\n",
    "Load libraries and extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from helpers import profile_gpu, profile_cpu, plot, overlay_plots\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create context and queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared local memory\n",
    "\n",
    "Shared Local Memory (SLM) is memory type that is placed closer to the processors performing operations. It is faster than global memory but usually there is not much of it available. You have to find your hardware specifications to get the actual numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQhMbskzhgt11jxSyvHDHnlD1pfTgUXIj2-_E0w905j5WxXRPBcl96NUKd7EY1uNzz-KvvyhOEPgS2D/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shared Local Memory can be a scarce resource and should be used thoughtfully. If used excessively it could have negative effect on the performance. As with all optimizations, resource usage is a balancing act.\n",
    "\n",
    "Fast SLM is located near the hardware executing work groups (on older Intel integrated GPUs it was located further from blocks performing calculations so it was quite slow). This hardware does not have access to other work groups. Through SLM information can be shared only between threads from the same Work Group Clusters (Streaming Multiprocessors or Subslices).\n",
    "\n",
    "Typical use cases:\n",
    "* caching access to global memory\n",
    "* storing and exchanging offsets or partial calculations\n",
    "* storing frequently accessed lookup tables\n",
    "\n",
    "In OpenCL SLM can be allocated in 2 ways:\n",
    "* statically from within the kernel - this way you have to specify the size of SLM at the compile time. In your kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void kernel_with_static_slm(const __global float *buffer)\n",
    "{\n",
    "    __local float localBuffer[1024];\n",
    "    \n",
    "    // some other code\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* dynamically - create a special LocalMemory type object in your host code, specifying the size of memory in bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_amount = 128\n",
    "local_size_in_bytes = 4 * threads_amount\n",
    "\n",
    "shared_local_memory = cl.LocalMemory(size=local_size_in_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object is then passed together with other arguments in the host code.\n",
    "\n",
    "In the kernel it's accessed through function arguments with **__local** qualifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void kernel_with_dynamic_slm(const __global float *globalbuffer, __local float *localBuffer)\n",
    "{   \n",
    "    const uint gid = get_global_id(0);\n",
    "    int some_value = localBuffer[gid];\n",
    "    // some other code\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Derivative Computation\n",
    "\n",
    "In this exercise we will write a code that computes a simple derivative. We will do it in two iterations. \n",
    "* first implement derivative computations using global memory\n",
    "* then transform the kernel to use Shared Local Memory\n",
    "\n",
    "\n",
    "A short recap from the university maths gives the following formula for derivative computations.\n",
    "\n",
    "$$\n",
    "y' = \\lim_{h \\to 0} \\frac{f(n + h) - f(n)}{h}\n",
    "$$\n",
    "\n",
    "So we compute values of the function at point x and at point very close to x. Then we subtract them and divide by change in x. Essentially what is computed is a rate of change on Y-axis to change in X-axis.\n",
    "\n",
    "This formula would take the following form in sofware developers language:\n",
    "\n",
    "$$\n",
    "y' = \\frac{y[n + 1] - y[n]}{x[n + 1] - x[n]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create points for some function - a cosine function was chosen semi-arbitrarily. It is a well know function and has the advantage that we know it's derviative, so we can compare the results computed in GPU with the actual values of the derivative. You can find more information about the derivation [here](https://www.cuemath.com/calculus/derivative-of-cosx/).\n",
    "\n",
    "* there is cosine function implemented in OpenCL, so we can call instrinsic function\n",
    "\n",
    "So we have the following formulas to work with:\n",
    "$$\n",
    "y = \\cos(x)\n",
    "\\newline\n",
    "y' = -\\sin(x)\n",
    "$$\n",
    "\n",
    "For nice visualization let's use a few periods. If you wish you can later experiments with different functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "step = np.float32(1e-4)\n",
    "x = np.arange(0, 8 * np.pi, step).astype(np.float32)\n",
    "f = lambda x: np.cos(x)\n",
    "y = f(x)\n",
    "print(f'We have {len(x)}, points to process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, 'x', 'cos(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know the derivative of the function we can also plot it. We will expect similar plot for points computed in GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_prime = lambda x: -np.sin(x)\n",
    "y_prime = f_prime(x)\n",
    "plot(x, y_prime, 'x', '-sin(x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Calculate the derviate in GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform the calculation in GPU we need to do the usual OpenCL setup. Create output buffer for datapoints of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "d_x = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=x)\n",
    "d_y_prime = cl.Buffer(ctx, flags.WRITE_ONLY, y.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to fix the kernel below to calculate the derviate according to the formula given above. So you need to:\n",
    "* compute function points at points x_1 and x_0\n",
    "* subtract the neighbouring values of f(x)\n",
    "* divide the difference by step\n",
    "* write the value back to the output buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "float f(float x)\n",
    "{\n",
    "    return cos(x);\n",
    "}\n",
    "\n",
    "__kernel void compute_derivative(const __global float *x,\n",
    "                                 __global float *y_prime,\n",
    "                                 int nr_elements)\n",
    "{\n",
    "    int thread_id = get_global_id(0);\n",
    "    int grid_stride = get_global_size(0);\n",
    "    \n",
    "    for(int i = thread_id; i < nr_elements; i+=grid_stride) {\n",
    "        float x_0 = x[i];\n",
    "        float x_1 = x[i + 1];\n",
    "        \n",
    "        // Fix the code below\n",
    "            \n",
    "        y_prime[i] = 0;        \n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some execution configuration. Threads in this exercise are reused, meaning that each will handle multiple points, looping through them using grid-stride pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_work_size = (64,)\n",
    "global_work_size = (local_work_size[0] * 128,)\n",
    "\n",
    "nr_output_elements = np.int32(len(x) - 1)\n",
    "\n",
    "print(f'local threads = {local_work_size[0]}, global threads = {global_work_size[0]}, {nr_output_elements} points to process.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule the execution with profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = profile_gpu(compute_derivative, 20,\n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            d_x,\n",
    "            d_y_prime,\n",
    "            nr_output_elements\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the data back from GPU. The plots should be overlaid and only the blue one should be visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(queue, d_buffer, nr_elements):\n",
    "    h_result = np.zeros(nr_elements + 1).astype(np.float32)\n",
    "    cl.enqueue_copy(queue, h_result, d_buffer)\n",
    "    \n",
    "    return h_result\n",
    "\n",
    "def compare(ground_truth, result, error_tolerance = 1e-3):\n",
    "    s = 0\n",
    "    e = len(result)\n",
    "\n",
    "    np.testing.assert_allclose(ground_truth[s:e], result[s:e], rtol=error_tolerance, atol=error_tolerance)\n",
    "\n",
    "h_y_prime_from_gpu = fetch_data(queue, d_y_prime, nr_output_elements)\n",
    "\n",
    "overlay_plots(x, y_prime, h_y_prime_from_gpu)\n",
    "\n",
    "compare(y_prime, h_y_prime_from_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [solution](./compute_derivative_global_memory.c) if you get stuck.\n",
    "\n",
    "### Performance constrains - Compute Bound vs Memory Bound\n",
    "\n",
    "There are two high level views on the performance bottlenecks which limit the execution latency:\n",
    "* Memory speed\n",
    "* Computation speed\n",
    "\n",
    "If our program is limited by accesses to the memory we say that it is \"Memory Bound\". In that case speeding up computations will not make any observable improvement to the time it takes to execute a kernel. Some examples of techniques to reduce memory load can be:\n",
    "* access memory is coelesced manner - which will utilize cache efficiently \n",
    "* explicitely cache data into SLM\n",
    "* access memory with large data types - eg. use uint4 to load data instead of four times uint\n",
    "* watch register usage - so how many variables you have declared in your kernel. If you exceed maximum number of registers available per thread, the compiler is forced to move the data to slower memory type and the performance hit is significant.\n",
    "\n",
    "If the program is limited by computations we say it is \"Compute Bound\". That can be the case for complex algorithms requiring lot of mathematical or binary operations. In that case data is loaded quickly but it takes a lot of GPU clocks to process it. Some examples of ways to tackle it are:\n",
    "* spread the computations between threads\n",
    "* avoid duplicating computations in each thread - use SLM to exachange computed data\n",
    "* use lookup tables for data that can be pre computed\n",
    "* use intrinsics if possible - functions provided with your framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synchronization and barriers\n",
    "\n",
    "Before diving into the code it's neccessary to explain a few key GPU concepts:\n",
    "* thread synchronization\n",
    "* shared local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQg7xzKDqvmAbPtl735zXtHc4itBbi7ar9jd6hHw9xt3zIsdBfSAna_DePFQa27TdeEyljFVKB3CEG_/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work groups usually consist of multiple sub groups. All threads within a subgroup execute the same instruction. However  subgroups, are not automatically synchronized with each other. They execute on different processors.\n",
    "\n",
    "More complex algorithms require some communication between threads - they require data calculated in one thread to be used in some other thread.\n",
    "\n",
    "Threads can exchange information through following mechanisms:\n",
    "* global memory\n",
    "* shared local memory\n",
    "* non-standard vendor specific extensions\n",
    "\n",
    "When one thread writes to a memory location and other threads reads from that place, we are exposed to a possible race condition. The value of that memory cell will depend on the order in which reading and writing threads accessed it. This problem should be familiar to you from virtually any multithreading programming environment.\n",
    "\n",
    "It's important to understand that there are only 2 kinds of explicit synchronization possible in GPU:\n",
    "* work group level synchronization - synchronizes threads from all sub groups within a group. When a barrier is placed somewhere in a kernel, threads within that group will wait at the barrier until all of them reach that execution point. At that place in a code we are sure that all operations until the barrier have completed - including operations writing to memory intended for other threads. To place a [barrier](https://www.khronos.org/registry/OpenCL/sdk/1.0/docs/man/xhtml/barrier.html) in your kernel code write:\n",
    "\n",
    "\n",
    "    - barrier(CLK_LOCAL_MEM_FENCE) - use with Shared Local Memory\n",
    "    - barrier(CLK_GLOBAL_MEM_FENCE) - use with global memory\n",
    "    - barrier(CLK_LOCAL_MEM_FENCE | CLK_GLOBAL_MEM_FENCE) - both\n",
    "\n",
    "\n",
    "* grid level synchronization - there is no way to synchronize work groups in a grid from kernel code. Work groups can be distributed across different Streaming multiprocessors (on Nvidia - groups of cores) or Subslices (on Intel - groups of Execution Units which group hardware threads - SIMDs - executing multiple software threads). The only way to synchronize work groups is through scheduling a separate kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Convert the kernel to use shared memory.\n",
    "\n",
    "Your next task is to convert the kernel computing function derivate to use shared local memory.\n",
    "\n",
    "The kernels we've written so far are Memory Bound. To demonstrate the effect of using SLM in such simple example we need to simulate complex computations - so we will artifically force the kernel to be Compute Bound. Calculating cosine function is very fast so actually using SLM in this case does not make sense, since each thread can quickly calculate neighboring values. However if we force the code to be compute bound (it's performance will be limited by computational complexity), then it makes sense to share data which is already computed by other thread. For that purpose function f was modified with a for-loop to take relatively significant amount of time to compute.\n",
    "\n",
    "As in the previous example, the code below computes the values of function f twice:\n",
    "* once for it's own thread\n",
    "* once for neighbouring thread\n",
    "However, since computing f is slow now this there is a performance hit.\n",
    "\n",
    "Your job is to:\n",
    "* add SLM to the kernel\n",
    "* Optimize the kernel to compute the value of f only once per thread\n",
    "* Share the results with the neighbouring threads.\n",
    "* don't forget to syncronize threads in a appropriate place\n",
    "\n",
    "This should speed up execution times by a factor of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "// This function is deliberately made inefficient to demonstrate some complex computations\n",
    "float f(float x)\n",
    "{\n",
    "    float y = 0;\n",
    "    // make sure the result is just cos(x) but slow the execution down\n",
    "    for (int i = -100; i < 1; i++)\n",
    "    {\n",
    "        y = cos(x + i);\n",
    "    }\n",
    "    return y;\n",
    "}\n",
    "\n",
    "\n",
    "__kernel void compute_derivative_slm(const __global float *x, \n",
    "                                     __global float *y_prime, \n",
    "                                     int nr_elements)\n",
    "{\n",
    "    const int lid = get_local_id(0);\n",
    "    const int local_size = get_local_size(0) - 1;\n",
    "    const int num_groups = get_num_groups(0);\n",
    "    const int group_id = get_group_id(0);\n",
    "    \n",
    "    const int grid_stride = num_groups * local_size;\n",
    "    const int thread_id = lid  + group_id * local_size;\n",
    "\n",
    "    for(int i = thread_id; i < nr_elements; i+=grid_stride) {\n",
    "        float x_0 = x[i];\n",
    "        float x_1 = x[i + 1];\n",
    "\n",
    "        \n",
    "        float y_0 = f(x_0);\n",
    "        float y_1 = f(x_1);\n",
    "        \n",
    "        if (lid < local_size)\n",
    "        {\n",
    "            y_prime[i] = (y_1 - y_0) / (x_1 - x_0);\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and schedule execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 256 * 64 * np.pi, step).astype(np.float32)\n",
    "\n",
    "local_work_size = (64,)\n",
    "global_work_size = (local_work_size[0] * 128,)\n",
    "\n",
    "nr_output_elements = np.int32(len(x) - 1)\n",
    "\n",
    "print(f'local threads = {local_work_size[0]}, global threads = {global_work_size[0]}, {nr_output_elements} points to process.')\n",
    "\n",
    "d_x = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=x)\n",
    "d_y_prime = cl.Buffer(ctx, flags.WRITE_ONLY, x.nbytes)\n",
    "\n",
    "profile_gpu(compute_derivative_slm, 20,\n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            d_x,\n",
    "            d_y_prime,\n",
    "            nr_output_elements\n",
    "            )\n",
    "\n",
    "h_y_prime_from_gpu = fetch_data(queue, d_y_prime, nr_output_elements)\n",
    "\n",
    "# plot and compare part of the result\n",
    "s = 0\n",
    "e = int(1000 * np.pi * 64)\n",
    "plot(x[s:e], h_y_prime_from_gpu[s:e], 'x', 'derivative (x)')\n",
    "\n",
    "y_prime = f_prime(x)\n",
    "compare(y_prime[s:e], h_y_prime_from_gpu[s:e])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE for inquisitive readers: the way to calculate grid_stride and thread_id was changed with respect to previous exercise. They both depend on ```local_size``` which is shrink by one thread to\n",
    "```local_size = get_local_size(0) - 1```.\n",
    "\n",
    "The reason for that is that we need to handle derivative calculation at the edge of work groups. To calculate derviate we need one more function value point than the actual size of work group - each thread need two points. We can tackle it the following ways:\n",
    "* skip last thread when computing derviative - and make following work-groups start one entry earlier\n",
    "* make one thread calculate function value twice\n",
    "\n",
    "The first option was chosen and it makes just one thread idle while the seconds makes mutliple threads idle.\n",
    "\n",
    "Refer to the [solution](./compute_derivative_solution_slm.c) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Exercise: Parallel Patterns - Reduction\n",
    "\n",
    "Reduction is very common parallel algorithm. It combines elements of an array into one element, using the same two-inputs one-output associative operator. \n",
    "\n",
    "Associative operator gives the same results no matter what the order of operations is.\n",
    "\n",
    "$$\n",
    "(a + b) + c  = a + (b + c) \n",
    "$$\n",
    "\n",
    "Examples or reductions of an array include:\n",
    "* calculating sum of all elements\n",
    "* finding maximum/minimum value\n",
    "* calculating value of a polynomial at a point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSv_IJe5in4xdBK1hLiXJ0idEZTl2S2nNsaQNCNgbPa6ES8u5k-M3AxrKjcMH6BVG3Q0VQnMLjjCr94/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction - summing in CPU\n",
    "\n",
    "To illustrate the reducion performance in host code we will have a look at summing array elements.\n",
    "\n",
    "Below are examples of summing elements of a collection in three ways:\n",
    "* serially - written in python 'by hand'\n",
    "* using build in 'sum'\n",
    "* using numpy sum which underneath the hood uses native compiled code\n",
    "\n",
    "NOTE: There is no GPU acceleration in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N=1024 * 256\n",
    "buffer = np.arange(0, N).astype(np.float32)\n",
    "\n",
    "def serial_sum(buffer):\n",
    "    value = 0\n",
    "    for i in buffer:\n",
    "        value += i\n",
    "    \n",
    "    return value\n",
    "    \n",
    "run_count = 10\n",
    "_ = profile_cpu(serial_sum, run_count, buffer)\n",
    "_ = profile_cpu(sum, run_count, buffer)\n",
    "_ = profile_cpu(np.sum, run_count, buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Which method is the fastest?\n",
    "2. Are the results consistent between all the methods? Why? What to do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce implementation in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQc4ae3fOz73VpylkfLm-I9xvOfGGU0cTjRG2tXiLMp5iPdQJ4lrxem9-XUAWOpycpZW06YVrU1Vqqw/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction Exercise \n",
    "\n",
    "Below is \"work in progress\" code which performs partial reduction in GPU. Each work group calculates a sum of all cell corresponding to this thread. The resulting sums are summed up in CPU.\n",
    "\n",
    "You task is to take over the code and improve it.\n",
    "1. Correct the code so that the kernel calculates partial sums correctly.\n",
    "    * You need to fix race condition - analyze reads and writes to temporary memory\n",
    "2. Use all available memory types - convert the kernel to use SLM:\n",
    "    * covert access IDs\n",
    "    * convert the barrier type\n",
    "    * allocate different memory type in host code\n",
    "3. Further optimize the code's performance:\n",
    "    * utilize threads effectively - make subgroups busy or idle as in the slides above\n",
    "    * make each thread busy at first iteration - currently first thread only loads data to temporary buffer\n",
    "   \n",
    "Notice how performance differs with the changes you make.\n",
    "\n",
    "For simplicity assume that the size of input data is always mulitple of work group size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "// Task 2 - consider all available memory types\n",
    "__kernel void reduce(const __global int *in_buf, __global int *temp, __global int *result)\n",
    "{\n",
    "    const uint gid = get_global_id(0);\n",
    "    const uint group_id = get_group_id(0);\n",
    "    const int local_size = get_local_size(0);\n",
    "    const int local_id = get_local_id(0);\n",
    "    \n",
    "    // Task 2 - what ids should be used to store and later retrieve intermediate results?\n",
    "    temp[gid] = in_buf[gid];\n",
    "\n",
    "    // Task 3 - Is this looping most convenient for calculations?\n",
    "    for (int step = 1; step < local_size; step *= 2)\n",
    "    {   \n",
    "        // Task 1 - fix synchronization\n",
    "        if (gid % (step * 2) == 0) // Task 3 - think which threads perform the actual addition\n",
    "        {\n",
    "            temp[gid] = temp[gid] + temp[gid + step];\n",
    "        }        \n",
    "    }\n",
    "    \n",
    "    if (local_id == 0) {\n",
    "        result[group_id] = temp[gid];\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data. For simplicity of reasoning we will:\n",
    "* Fill the input buffer with ones, so effectively the reduction will calculate number of elements\n",
    "* Have one thread per one buffer element (subject to optimization)\n",
    "\n",
    "Please keep in mind that we do not handle integer precision overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_type = np.int32\n",
    "N=67108864\n",
    "M=128\n",
    "global_work_size = (N, )\n",
    "local_work_size = (M, )\n",
    "num_groups = N // M\n",
    "\n",
    "h_buffer = np.ones(N).astype(buffer_type)\n",
    "\n",
    "print (f\"local_work_size = {local_work_size} global_work_size = {global_work_size} Nr Work Groups = {num_groups}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create input and output buffers. Please note that the algorithm requires writing data back to some intermediate buffer. As a starting point it will be global memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "d_input_buffer = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_buffer)\n",
    "\n",
    "# Task 2 - is this memory the best for sharing data between threads?\n",
    "d_intermediate_buffer = cl.Buffer(ctx, flags.READ_WRITE, h_buffer.nbytes)\n",
    "\n",
    "d_result = cl.Buffer(ctx, flags.WRITE_ONLY, 4 * num_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule kernel execution and fetch the data back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_time_ms = profile_gpu(reduce, 50,\n",
    "                          queue, \n",
    "                          global_work_size, \n",
    "                          local_work_size,\n",
    "                          d_input_buffer,\n",
    "                          d_intermediate_buffer,\n",
    "                          d_result\n",
    "                          )\n",
    "\n",
    "h_result = np.zeros(num_groups).astype(np.int32)\n",
    "_ = cl.enqueue_copy(queue, h_result, d_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results with values computed by numpy. Please note that results computed in GPU are partial. There are multiple solutions to tackle that:\n",
    "* schedule another kernel to compute the remaining sum of sums\n",
    "* compute remaining work in CPU\n",
    "\n",
    "Since the purpose of this exercise is to demonstrate the usage of SLM we will go for just summing the remining values in CPU. We are going to profile this sum as well to see if the overall hybrid CPU + GPU solutions still makes sense from latency perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_opencl, sum_partials_ms = profile_cpu(np.sum, 20, h_result)\n",
    "\n",
    "res_numpy, numpy_ms = profile_cpu(np.sum, 20, h_buffer)\n",
    "\n",
    "np.testing.assert_equal(res_opencl, res_numpy)\n",
    "print (f\"gpu   reduction summed value = {res_opencl} median time took {gpu_time_ms:.4f} ms, sum partial results took {sum_partials_ms:.4f} ms\")\n",
    "print (f\"numpy reduction summed value = {res_numpy} median time took {numpy_ms:.4f} ms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "* How does execution time change with every optimization?\n",
    "* Which optimizations were beneficial?\n",
    "* Can you explain why there are differences?\n",
    "\n",
    "If you get stuck:\n",
    "* [Part 1 solution](./reduction_part1_solution.c)\n",
    "* [Part 2 solution](./reduction_part2_solution.c)\n",
    "* [Part 3 solution](./reduction_part3_solution.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQhdPev2THHUBZD8ORyBFgZwOT_2ST-yQe17Zv00eIkWQkS9U1rQNXgXmsXMzJLL9L301r1tkhlqrZQ/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are welcome to experiment with this and other optimization on a other platforms - perhaps on your own machine.\n",
    "\n",
    "## Curiosities\n",
    "\n",
    "* In OpenCL 2.0 there was a kernel function added [work_group_reduce_add](https://www.khronos.org/registry/OpenCL/sdk/2.1/docs/man/xhtml/work_group_reduce.html). As the same suggests it does what our reduce kernel does. There is actually a whole family of reduce functions.\n",
    "\n",
    "* A bunch of other useful work group level functions were added - you can read about it [here](https://software.intel.com/en-us/articles/using-opencl-20-work-group-functions).\n",
    "\n",
    "* For floating point operations order matters - so it is not assosiative operation\n",
    "* There is a limited precision in 32 bits numbers. In case of large sum this can quickly overflow.\n",
    "\n",
    "## Extra task\n",
    "\n",
    "Convert reduction kernel to use intrinsic function [work_group_reduce_add](https://www.khronos.org/registry/OpenCL/sdk/2.1/docs/man/xhtml/work_group_reduce.html) to perform reduction. Observe further reduction in execution time and how the code is simplified.\n",
    "\n",
    "If you get stuck solution is below.\n",
    "\n",
    "* [Intrinsics solution](./reduction_intrinsic_solution.c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
