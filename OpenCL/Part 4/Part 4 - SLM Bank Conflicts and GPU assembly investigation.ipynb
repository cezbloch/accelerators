{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Local Memory Bank Conflicts\n",
    "\n",
    "This notebook describes an advanced technique to optimize SLM access patterns.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Load libraries and extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from helpers import profile_gpu\n",
    "\n",
    "os.environ[\"PYOPENCL_COMPILER_OUTPUT\"] = \"1\" # set to 1 to see compiler warnings\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create context and queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Banks\n",
    "\n",
    "The slides below describe what are Memory Banks and how to use access SLM avoiding conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vSlVEew--oxKhSXYcCJP3vRHD2EQ-gPYH1g7lt0FotTBAV2LzbRF0koXVXRXIpJkv920L0rcqVSrbzz/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"960\" height=\"569\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information here is a good description on [Bank Conflicts on NVidia](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x).\n",
    "\n",
    "## Exercise - Remove memory bank conflicts\n",
    "\n",
    "The code below executes some fictional calculations, which suffer from Memory Bank Conflicts. \n",
    "\n",
    "First, define input data. We'll just use an array of same numbers and will increment them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = np.int32(2**25)\n",
    "h_a = np.full(N, 1).astype(np.int32)\n",
    "\n",
    "print(f\"Working with {len(h_a):,} elements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define execution configuration and buffers. We're going to add extra buffers to:\n",
    "* store number of GPU clock cycles spend by a work group - so we can see the cost of accessing SLM\n",
    "* store time spend in GPU\n",
    "* store Streaming multiprocessor IDs - not related to Bank conflicts - but as a curiosity so that we can see how work groups are scheduled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "\n",
    "local_work_size = (32,)\n",
    "global_work_size = (N,)\n",
    "num_groups = global_work_size[0]//local_work_size[0]\n",
    "\n",
    "d_a = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_a)\n",
    "d_result = cl.Buffer(ctx, flags.WRITE_ONLY, h_a.nbytes)\n",
    "d_clock_cycles = cl.Buffer(ctx, flags.WRITE_ONLY, num_groups * 4)\n",
    "d_durations_ns = cl.Buffer(ctx, flags.WRITE_ONLY, num_groups * 4)\n",
    "d_smids = cl.Buffer(ctx, flags.WRITE_ONLY, num_groups * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a kernel with some profiling function calls. There is a trick in OpenCL to use CUDA calls directly using assemler instructions.\n",
    "\n",
    "* [clock function](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#time-function) - returns the current clock counter\n",
    "* [globaltimer_lo](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers-globaltimer) - returns lower 32bits of GPU timer in nanoseconds\n",
    "* [smid](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers-smid) - returns on ID on Streaming multiprocessor on which a thread is executing\n",
    "* [all special registers](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#special-registers) - list of all available registers\n",
    "\n",
    "We've also defined 'SomeInfo' structure which is used to stored some information. In this example it's just declared for demonstration purposes, but it's easy to assume some other useful data could be stored in it, in a real example.\n",
    "\n",
    "In the kernel below we fetch data from global memory, put it into SLM and increment 100 times, so that SLM is used multiple times and the results of such calculations are visible.\n",
    "\n",
    "At the end, first thread in a work group saves a few profiling informations as well as information on which SM this group executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "uint clock()\n",
    "{\n",
    "    uint clock_time;\n",
    "    asm volatile (\"mov.u32 %0, %%clock;\" : \"=r\"(clock_time));\n",
    "    return clock_time;\n",
    "}\n",
    "\n",
    "uint globaltimer_ns()\n",
    "{\n",
    "    uint timer;\n",
    "    asm volatile (\"mov.u32 %0, %%globaltimer_lo;\" : \"=r\"(timer));\n",
    "    return timer;\n",
    "}\n",
    "\n",
    "uint get_smid(void) \n",
    "{\n",
    "     uint ret;\n",
    "     asm(\"mov.u32 %0, %%smid;\" : \"=r\"(ret) );\n",
    "     return ret;\n",
    "}\n",
    "\n",
    "typedef struct\n",
    "{\n",
    "    int a;\n",
    "    int buffer_data[31];\n",
    "} SomeInfo;\n",
    "\n",
    "__kernel void add_vectors(__global const int *a, \n",
    "                          __global int *result,  \n",
    "                          __global uint* clock_cycles,\n",
    "                          __global uint* durations,\n",
    "                          __global uint* sms)\n",
    "{\n",
    "    __local SomeInfo slm[32];\n",
    "    \n",
    "    uint start_timer = globaltimer_ns();\n",
    "   \n",
    "    int gid = get_global_id(0);\n",
    "    int lid = get_local_id(0);\n",
    "    int group_id = get_group_id(0);\n",
    "    int local_size = get_local_size(0);\n",
    "    \n",
    "    slm[lid].a = a[gid];\n",
    "    \n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    \n",
    "    uint start_cycles = clock();\n",
    "        \n",
    "    for (int i=0; i < 100; i++) {\n",
    "        slm[lid].a++;\n",
    "    }\n",
    "\n",
    "    uint end_cycles = clock();\n",
    "    \n",
    "    result[gid] = slm[lid].a;\n",
    "    \n",
    "    uint end_timer = globaltimer_ns();\n",
    "    \n",
    "    if (lid == 0) {\n",
    "        clock_cycles[group_id] = end_cycles - start_cycles;\n",
    "        durations[group_id] = end_timer - start_timer;\n",
    "        sms[group_id] = get_smid();\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedule work to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = profile_gpu(add_vectors, 20, \n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            d_a,\n",
    "            d_result,\n",
    "            d_clock_cycles,\n",
    "            d_durations_ns,\n",
    "            d_smids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch data back to CPU and display as Pandas Dataframe. Pandas is a useful library for visualizing data and statistics. It's also popular in Machine Learning applications, but here we'll just use it to display data in Excel-like manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h_result = np.zeros(N).astype(np.int32)\n",
    "h_clock_cycles = np.zeros(num_groups).astype(np.uint32)\n",
    "h_durations_ns = np.zeros(num_groups).astype(np.uint32)\n",
    "h_smids = np.zeros(num_groups).astype(np.int32)\n",
    "\n",
    "cl.enqueue_copy(queue, h_result, d_result)\n",
    "cl.enqueue_copy(queue, h_clock_cycles, d_clock_cycles)\n",
    "cl.enqueue_copy(queue, h_durations_ns, d_durations_ns)\n",
    "cl.enqueue_copy(queue, h_smids, d_smids)\n",
    "\n",
    "df = pd.DataFrame({'clock cycles' : h_clock_cycles, \n",
    "                   'Duration [ns]': h_durations_ns,\n",
    "                   'Streaming Multiprocessor IDs' : h_smids,\n",
    "                   'Result' : h_result[::32],})\n",
    "\n",
    "df.index.name = 'Work Group ID'\n",
    "pd.set_option('display.max_rows', 500)\n",
    "df[:64] # display only some work groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Your task is to bring down the execution time and number of clock cycles spend accessing SLM, using knowledge gain in the presentation above.\n",
    "\n",
    "Refer to the [solution](./slm_bank_conflicts_solution.c) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank conflicts with global and private memory\n",
    "\n",
    "So why are there Bank conflicts in SLM but not in registers or global memory?\n",
    "\n",
    "### Global memory\n",
    "Accesses to global memory are much slower and are grouped into requests, so they are not visible. Global memory is accessed by 64 or 128 byte long requests. If multiple threads access data within this transfer you get good performance because of coelesced accesses. This has been discussed in details in previous notebooks.\n",
    "\n",
    "### Private memory - registers\n",
    "\n",
    "Registers are private to each thread so there are no bank conflicts. It's just one thread accessing this type of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating GPU assembly \n",
    "\n",
    "When digging deeper in search for performance you may want to look at the assembly instructions generated by the compiler. You will be able to see exactly what you code does.  Details of [Nvidia PTX assembly](https://docs.nvidia.com/cuda/inline-ptx-assembly/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernel_string = \"\"\"\n",
    "uint clock_time()\n",
    "{\n",
    "    uint clock_time;\n",
    "    asm volatile(\"mov.u64 %0, %%globaltimer;\" : \"=l\"(clock_time));\n",
    "    return clock_time;\n",
    "}\n",
    "\n",
    "uint get_smid(void) \n",
    "{\n",
    "     uint ret;\n",
    "     asm(\"mov.u32 %0, %%smid;\" : \"=r\"(ret) );\n",
    "     return ret;\n",
    "}\n",
    "\n",
    "__kernel void add_vectors(__global const int *a, \n",
    "                          __global const int *b, \n",
    "                          __global int *c,  \n",
    "                          __global uint* times,\n",
    "                          __global uint* sms)\n",
    "{\n",
    "    int gid = get_global_id(0);\n",
    "    int lid = get_local_id(0);\n",
    "    int group_id = get_group_id(0);\n",
    "    barrier(CLK_GLOBAL_MEM_FENCE);\n",
    "    uint start = clock_time();\n",
    "    barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    \n",
    "    c[gid] = 0;\n",
    "    \n",
    "    for (int i=0; i < 100; i++) {\n",
    "        c[gid] += i * a[gid] + b[gid];        \n",
    "    }\n",
    "    \n",
    "    uint end = clock_time();\n",
    "    \n",
    "    times[group_id] = end - start;\n",
    "    \n",
    "    if (lid == 0) {    \n",
    "        sms[group_id] = get_smid();\n",
    "    }\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prg = cl.Program(ctx, kernel_string).build()\n",
    "\n",
    "print(prg.binaries[0].decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Nvidia the code above will the following instructions:\n",
    "* ld.param.u64 - loading kernel parameters\n",
    "* bar.sync - barrier\n",
    "* mov.u32 - assignment operations eg. \n",
    "    * mov.u32 %r11, %ntid.x; - will move conent of 'ntid' register to 11th register 'r11'. 'ntid' contains thread ID\n",
    "* st.global.u32 - store to global memory\n",
    "* add - addition operation\n",
    "* mul - multiply operation\n",
    "* mad - multiply and add operation - two operations in one cycle\n",
    "* BB0_1 - branching - in this case for-loop, can also be if-statement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
