{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de3f3f",
   "metadata": {},
   "source": [
    "# Matrix Multiplication with PyOpenCL\n",
    "\n",
    "This notebook demonstrates how to perform matrix multiplication using PyOpenCL. The OpenCL kernel implementation is left empty for you to complete as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from helpers import profile_gpu\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1df621",
   "metadata": {},
   "source": [
    "Create context and queue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ff35060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=<pyopencl.Device 'NVIDIA GeForce RTX 2060 with Max-Q Design' on 'NVIDIA CUDA' at 0x19d3cdb2bf0>\n"
     ]
    }
   ],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d62724",
   "metadata": {},
   "source": [
    "## Define Matrices\n",
    "\n",
    "Let's define two matrices to multiply. You can change their size and values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d28e2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matrices A and B\n",
    "N = 1024\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb91e16",
   "metadata": {},
   "source": [
    "## Perform Matrix Multiplication on the GPU\n",
    "\n",
    "We will multiply matrices A and B using an OpenCL kernel. Implement the kernel code to perform standard matrix multiplication as done in algebra. For two matrices A (of size m×n) and B (of size n×p), their product C = AB is an m×p matrix where each element C[i, j] is computed as the sum of products of the i-th row of A and the j-th column of B:\n",
    "\n",
    "$$\n",
    "C_{i,j} = \\sum_{k=1}^{N} A_{i,k} \\cdot B_{k,j}\n",
    "$$\n",
    "\n",
    "\n",
    "This operation is also known as \"GEMM\" (General Matrix Multiply) in numerical computing libraries.\n",
    "\n",
    "We will build-up the optimal solution in stages. Start simple, and implement the multiplication using just global memory, where each work item computes output value for one cell in the output matrix.\n",
    "\n",
    "You can assume that all matrices are square for now, if you find it more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61d9be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenCL context and buffers\n",
    "mf = cl.mem_flags\n",
    "d_a = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)\n",
    "d_b = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)\n",
    "d_c = cl.Buffer(ctx, mf.WRITE_ONLY, A.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d7a9c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\accelerators\\.venv\\Lib\\site-packages\\pyopencl\\__init__.py:570: CompilerWarning: Non-empty compiler output encountered. Set the environment variable PYOPENCL_COMPILER_OUTPUT=1 to see more.\n",
      "  lambda: self._prg.build(options_bytes, devices),\n"
     ]
    }
   ],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "__kernel void matmul(__global float* A, __global float* B, __global float* C, int N) {\n",
    "    float sum = 0;\n",
    "    int2 global_id = (int2)(get_global_id(0), get_global_id(1));\n",
    "\n",
    "    if (global_id.x >= N || global_id.y >= N) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // your code goes here\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        int aij = global_id.y * N + i;\n",
    "        int bij = i * N + global_id.x;\n",
    "        sum  += A[aij] * B[bij];\n",
    "    }\n",
    "\n",
    "    int cij = global_id.y * N + global_id.x;\n",
    "    C[cij] = sum;    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2087f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching with global_size=(1024, 1024), block_size=(8, 8)\n",
      "matmul took minimum = 11.5420 ms, on average 18.6876 ms, with median 11.5600 ms, variance 155.7592 ms, standard deviation 12.4804 ms.\n",
      "matmul took minimum = 11.5420 ms, on average 18.6876 ms, with median 11.5600 ms, variance 155.7592 ms, standard deviation 12.4804 ms.\n"
     ]
    }
   ],
   "source": [
    "# Launch kernel\n",
    "block_size = (8, 8)\n",
    "global_size = (A.shape[0], A.shape[1])\n",
    "print(f'Launching with global_size={global_size}, block_size={block_size}')\n",
    "n_warmup = 2\n",
    "n_iters = 50\n",
    "\n",
    "_ = profile_gpu(matmul, 20,\n",
    "            queue, \n",
    "            global_size, \n",
    "            block_size,\n",
    "            d_a,\n",
    "            d_b,\n",
    "            d_c,\n",
    "            np.int32(N)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2f5e2",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "After running the kernel, copy the result back to the host and display it.\n",
    "Refer to the solution if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2830b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result from GPU and display\n",
    "C = np.empty_like(A)\n",
    "cl.enqueue_copy(queue, C, d_c)\n",
    "c_numpy = np.matmul(A, B)\n",
    "#print(\"Result matrix C (A x B):\", C)\n",
    "np.testing.assert_almost_equal(C, c_numpy, decimal=3)\n",
    "# Note: You need to implement the kernel for correct results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a8946",
   "metadata": {},
   "source": [
    "# Shared memory caching\n",
    "\n",
    "Your next task is to optimize the kernel execution time. One of the common practices in matrix multiplication is to optimize memory accesses. Elements in each input matrices A and B are accessed N times by reaching to global memory. While we can expect some level of caching happening under the hood, we can also cache parts of A and B matrices in local memory (OpenCL's __local).\n",
    "\n",
    "Below is a helper function that will launch your new kernel. It will:\n",
    "* validate the sizes of your matrices\n",
    "* automatically allocate dynamic local memory for matrices A and B\n",
    "* transfer matrices from host to device and back\n",
    "* launch your kernel\n",
    "* verify correctness of the calculations against numpy implementation\n",
    "* if the results are correct it will launch the kernel multiple times to measure execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_matrix_multiply(A, B, kernel_code, block_size=(16, 16), warmup=2, iters=100, *args):\n",
    "    import numpy as np\n",
    "    import pyopencl as cl\n",
    "    from OpenCL.helpers import profile_gpu\n",
    "    M, N_A = A.shape\n",
    "    N_B, P = B.shape\n",
    "    print(f\"A: MxN ({M}, {N_A}), B: NxP ({N_B}, {P}), C: MxP ({M}, {P})\")\n",
    "    assert N_A == N_B, 'Inner matrix dimensions must match'\n",
    "    ctx = cl.create_some_context()\n",
    "    queue = cl.CommandQueue(ctx)\n",
    "    mf = cl.mem_flags\n",
    "    d_a = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)\n",
    "    d_b = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)\n",
    "    d_c = cl.Buffer(ctx, mf.WRITE_ONLY, M * P * 4)\n",
    "    prg = cl.Program(ctx, kernel_code).build()\n",
    "    global_size = (P, M)\n",
    "    local_size = block_size\n",
    "    use_local = '__local' in kernel_code\n",
    "    shared_mem_bytes = block_size[0] * block_size[1] * 4 * 2 if use_local else 0\n",
    "    def launch():\n",
    "        prg.matmul(queue, global_size, local_size, d_a, d_b, d_c, *args)  # add local memory if needed\n",
    "    print(f'Launching with global_size={global_size}, local_size={local_size}, shared_mem_bytes={shared_mem_bytes}')\n",
    "    launch()\n",
    "    C = np.empty((M, P), dtype=np.float32)\n",
    "    cl.enqueue_copy(queue, C, d_c)\n",
    "    ref = np.matmul(A, B)\n",
    "    np.testing.assert_almost_equal(C, ref, decimal=3)\n",
    "    _ = profile_gpu(launch, n_warmup=warmup, n_iters=iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bce1d",
   "metadata": {},
   "source": [
    "## Optimization task\n",
    "\n",
    "Write an optimized kernel which will cache blocks from A and B matrices in local memory:\n",
    "- for simplicity assume that the shapes of all matrices are multiple of block size\n",
    "- make sure there are not data races - so synchronize local accesses\n",
    "\n",
    "Implement the following algorithm:\n",
    "- for each cache block\n",
    "    - load 8x8 blocks from global memory into declared local memory\n",
    "        - the main difficulty lays in accessing global memory based while iterating over cached blocks\n",
    "    - produce partial matrix multiplication sum from data cached in local memory and accumulate in local variable\n",
    "        - here you just need local ids\n",
    "- dump accumulated sum into global memory C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_mem_kernel = \"\n",
    "\n",
    "__kernel void matmul(__global float* A, __global float* B, __global float* C, int N, __local float* slm_A, __local float* slm_B) {\n",
    "    int row = get_global_id(0);\n",
    "    int col = get_global_id(1);\n",
    "    int local_row = get_local_id(0);\n",
    "    int local_col = get_local_id(1);\n",
    "    // your code goes here\n",
    "}\n",
    "\"\n",
    "\n",
    "args = (np.int32(N),)\n",
    "gpu_matrix_multiply(A, B, shared_mem_kernel, (16, 16), 2, 100, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab9fcd",
   "metadata": {},
   "source": [
    "Refer to the solution if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a0b90",
   "metadata": {},
   "source": [
    "# Non-square matrices\n",
    "\n",
    "Adapt your solution to work with non-square matrices. You will need to:\n",
    "* pass sizes of matrices to your kernel - as they are non-square so can have different shapes than only N.\n",
    "* Use these shapes in your kernel to access elements.\n",
    "* think about iterating through consecutive cached blocks in the for-loop. Make sure to look iterate from the perspective of output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b359fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_square_kernel = \"\n",
    "\n",
    "__kernel void matmul(__global float* A, __global float* B, __global float* C, int M, int N, int P, __local float* slm_A, __local float* slm_B) {\n",
    "    int row = get_global_id(0);\n",
    "    int col = get_global_id(1);\n",
    "    // your code goes here\n",
    "}\n",
    "\"\n",
    "\n",
    "A_ = np.random.randn(512, 1024).astype(np.float32)\n",
    "B_ = np.random.randn(1024, 768).astype(np.float32)\n",
    "M, N, P = A_.shape[0], A_.shape[1], B_.shape[1]\n",
    "args = (np.int32(M), np.int32(N), np.int32(P))\n",
    "gpu_matrix_multiply(A_, B_, non_square_kernel, (16, 16), 2, 100, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646c596",
   "metadata": {},
   "source": [
    "Refer to the solution if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
