{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2de3f3f",
   "metadata": {},
   "source": [
    "# Matrix Multiplication with PyOpenCL\n",
    "\n",
    "This notebook demonstrates how to perform matrix multiplication using PyOpenCL. The OpenCL kernel implementation is left empty for you to complete as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32b483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "os.environ[\"PYOPENCL_COMPILER_OUTPUT\"] = \"1\"\n",
    "\n",
    "from helpers import profile_gpu\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1df621",
   "metadata": {},
   "source": [
    "Create context and queue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff35060",
   "metadata": {},
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d62724",
   "metadata": {},
   "source": [
    "## Define Matrices\n",
    "\n",
    "Let's define two matrices to multiply. You can change their size and values as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28e2259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two matrices A and B\n",
    "N = 1024\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb91e16",
   "metadata": {},
   "source": [
    "## Perform Matrix Multiplication on the GPU\n",
    "\n",
    "We will multiply matrices A and B using an OpenCL kernel. Implement the kernel code to perform standard matrix multiplication as done in algebra. For two matrices A (of size m×n) and B (of size n×p), their product C = AB is an m×p matrix where each element C[i, j] is computed as the sum of products of the i-th row of A and the j-th column of B:\n",
    "\n",
    "$$\n",
    "C_{i,j} = \\sum_{k=1}^{N} A_{i,k} \\cdot B_{k,j}\n",
    "$$\n",
    "\n",
    "\n",
    "This operation is also known as \"GEMM\" (General Matrix Multiply) in numerical computing libraries.\n",
    "\n",
    "We will build-up the optimal solution in stages. Start simple, and implement the multiplication using just global memory, where each work item computes output value for one cell in the output matrix.\n",
    "\n",
    "You can assume that all matrices are square for now, if you find it more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenCL context and buffers\n",
    "mf = cl.mem_flags\n",
    "d_a = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)\n",
    "d_b = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)\n",
    "d_c = cl.Buffer(ctx, mf.WRITE_ONLY, A.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "__kernel void matmul(__global float* A, __global float* B, __global float* C, int N) {\n",
    "    float sum = 0;\n",
    "    int2 global_id = (int2)(get_global_id(0), get_global_id(1));\n",
    "\n",
    "    if (global_id.x >= N || global_id.y >= N) {\n",
    "        return;\n",
    "    }\n",
    "    \n",
    "    // your code goes here\n",
    "    \n",
    "    for (int i = 0; i < N; i++) {\n",
    "        int aij = global_id.y * N + i;\n",
    "        int bij = i * N + global_id.x;\n",
    "        sum  += A[aij] * B[bij];\n",
    "    }\n",
    "\n",
    "    int cij = global_id.y * N + global_id.x;\n",
    "    C[cij] = sum;    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2087f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch kernel\n",
    "block_size = (8, 8)\n",
    "global_size = (A.shape[0], A.shape[1])\n",
    "print(f'Launching with global_size={global_size}, block_size={block_size}')\n",
    "n_warmup = 2\n",
    "n_iters = 50\n",
    "\n",
    "_ = profile_gpu(matmul, 20,\n",
    "            queue, \n",
    "            global_size, \n",
    "            block_size,\n",
    "            d_a,\n",
    "            d_b,\n",
    "            d_c,\n",
    "            np.int32(N)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2f5e2",
   "metadata": {},
   "source": [
    "## Verify Results\n",
    "\n",
    "After running the kernel, copy the result back to the host and display it.\n",
    "\n",
    "Refer to the [solution](matmul_global_solution.c) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy result from GPU and display\n",
    "C = np.empty_like(A)\n",
    "cl.enqueue_copy(queue, C, d_c)\n",
    "c_numpy = np.matmul(A, B)\n",
    "#print(\"Result matrix C (A x B):\", C)\n",
    "np.testing.assert_almost_equal(C, c_numpy, decimal=3)\n",
    "# Note: You need to implement the kernel for correct results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a8946",
   "metadata": {},
   "source": [
    "# Shared memory caching\n",
    "\n",
    "Your next task is to optimize the kernel execution time. One of the common practices in matrix multiplication is to optimize memory accesses. Elements in each input matrices A and B are accessed N times by reaching to global memory. While we can expect some level of caching happening under the hood, we can also cache parts of A and B matrices in local memory (OpenCL's __local).\n",
    "\n",
    "Below is a helper function that will launch your new kernel. It will:\n",
    "* validate the sizes of your matrices\n",
    "* automatically allocate dynamic local memory for matrices A and B\n",
    "* transfer matrices from host to device and back\n",
    "* launch your kernel\n",
    "* verify correctness of the calculations against numpy implementation\n",
    "* if the results are correct it will launch the kernel multiple times to measure execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937c8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_matrix_multiply(A, B, kernel, block_size=(16, 16), warmup=2, iters=100, *args):\n",
    "    M, N_A = A.shape\n",
    "    N_B, P = B.shape\n",
    "    print(f\"A: MxN ({M}, {N_A}), B: NxP ({N_B}, {P}), C: MxP ({M}, {P})\")\n",
    "    assert N_A == N_B, 'Inner matrix dimensions must match'\n",
    "\n",
    "    mf = cl.mem_flags\n",
    "    d_a = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A)\n",
    "    d_b = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B)\n",
    "    d_c = cl.Buffer(ctx, mf.WRITE_ONLY, M * P * 4)\n",
    "\n",
    "    global_size = (P, M)\n",
    "    local_size = block_size\n",
    "    shared_mem_bytes = block_size[0] * block_size[1] * 4 #if use_local else 0\n",
    "    d_slm_a = cl.LocalMemory(shared_mem_bytes)\n",
    "    d_slm_b = cl.LocalMemory(shared_mem_bytes)\n",
    "    def launch():\n",
    "        kernel(queue, global_size, local_size, d_a, d_b, d_c, d_slm_a, d_slm_b, *args)\n",
    "\n",
    "    print(f'Launching with global_size={global_size}, local_size={local_size}, 2 x shared_mem_bytes={2 * shared_mem_bytes}')\n",
    "\n",
    "    launch()\n",
    "\n",
    "    C = np.empty((M, P), dtype=np.float32)\n",
    "    cl.enqueue_copy(queue, C, d_c)\n",
    "\n",
    "    ref = np.matmul(A, B)\n",
    "    np.testing.assert_almost_equal(C, ref, decimal=3)\n",
    "\n",
    "    _ = profile_gpu(kernel, iters, queue, global_size, local_size, d_a, d_b, d_c, d_slm_a, d_slm_b, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bce1d",
   "metadata": {},
   "source": [
    "## Optimization task\n",
    "\n",
    "Write an optimized kernel which will cache blocks from A and B matrices in local memory:\n",
    "- for simplicity assume that the shapes of all matrices are multiple of block size\n",
    "- make sure there are not data races - so synchronize local accesses\n",
    "\n",
    "Implement the following algorithm:\n",
    "- for each cache block\n",
    "    - load 8x8 blocks from global memory into declared local memory\n",
    "        - the main difficulty lays in accessing global memory based while iterating over cached blocks\n",
    "    - produce partial matrix multiplication sum from data cached in local memory and accumulate in local variable\n",
    "        - here you just need local ids\n",
    "- dump accumulated sum into global memory C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1669e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "#define THREAD_INDEX (get_local_id(1) * get_local_size(0) + get_local_id(0))\n",
    "\n",
    "__kernel void shared_mem_kernel(__global float *A,\n",
    "                     __global float *B,\n",
    "                     __global float *C, \n",
    "                    __local float* slm_A, \n",
    "                    __local float* slm_B,\n",
    "                    int N)\n",
    "{\n",
    "    float sum = 0.0f;\n",
    "    int2 global_id = (int2)(get_group_id(0) * get_local_size(0) + get_local_id(0),\n",
    "                            get_group_id(1) * get_local_size(1) + get_local_id(1));\n",
    "\n",
    "    for (int b = 0; b < get_num_groups(0); b++) {\n",
    "        int2 gidA = (int2)(b * get_local_size(0) + get_local_id(0), global_id.y);\n",
    "        int2 gidB = (int2)(global_id.x, b * get_local_size(1) + get_local_id(1));\n",
    "\n",
    "        slm_A[THREAD_INDEX] = A[gidA.y * N + gidA.x];\n",
    "        slm_B[THREAD_INDEX] = B[gidB.y * N + gidB.x];\n",
    "\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "        for (int i = 0; i < get_local_size(0); i++) {\n",
    "            sum += slm_A[get_local_size(0) * get_local_id(1) + i] *\n",
    "                   slm_B[get_local_size(0) * i + get_local_id(0)];\n",
    "        }\n",
    "\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    }\n",
    "\n",
    "    C[global_id.y * N + global_id.x] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b71db07",
   "metadata": {},
   "source": [
    "Execute the kernel with SLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (np.int32(N),)\n",
    "gpu_matrix_multiply(A, B, shared_mem_kernel, (16, 16), 2, 100, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ab9fcd",
   "metadata": {},
   "source": [
    "Refer to the [solution](matmul_slm_solution.c) if you get stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a0b90",
   "metadata": {},
   "source": [
    "# Non-square matrices\n",
    "\n",
    "Adapt your solution to work with non-square matrices. You will need to:\n",
    "* pass sizes of matrices to your kernel - as they are non-square so can have different shapes than only N.\n",
    "* Use these shapes in your kernel to access elements.\n",
    "* think about iterating through consecutive cached blocks in the for-loop. Make sure to look iterate from the perspective of output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67532a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel\n",
    "\n",
    "#define THREAD_INDEX (get_local_id(1) * get_local_size(0) + get_local_id(0))\n",
    "\n",
    "__kernel void non_square_matmul(__global float *A,\n",
    "                                __global float *B,\n",
    "                                __global float *C, \n",
    "                                __local float* slm_A, \n",
    "                                __local float* slm_B,\n",
    "                                int M,\n",
    "                                int N,\n",
    "                                int P)\n",
    "{\n",
    "    float sum = 0.0f;\n",
    "    int2 global_id = (int2)(get_group_id(0) * get_local_size(0) + get_local_id(0),\n",
    "                            get_group_id(1) * get_local_size(1) + get_local_id(1));\n",
    "\n",
    "    int nr_blocks = (N + get_local_size(0) - 1) / get_local_size(0);\n",
    "        \n",
    "    for (int b = 0; b < nr_blocks; b++) {\n",
    "        int2 gidA = (int2)(b * get_local_size(0) + get_local_id(0), global_id.y);\n",
    "        int2 gidB = (int2)(global_id.x, b * get_local_size(1) + get_local_id(1));\n",
    "\n",
    "        slm_A[THREAD_INDEX] = A[gidA.y * N + gidA.x];\n",
    "        slm_B[THREAD_INDEX] = B[gidB.y * P + gidB.x];\n",
    "\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "\n",
    "        for (int i = 0; i < get_local_size(0); i++) {\n",
    "            sum += slm_A[get_local_size(0) * get_local_id(1) + i] *\n",
    "                   slm_B[get_local_size(0) * i + get_local_id(0)];\n",
    "        }\n",
    "\n",
    "        barrier(CLK_LOCAL_MEM_FENCE);\n",
    "    }\n",
    "\n",
    "    C[global_id.y * P + global_id.x] = sum;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af324bb",
   "metadata": {},
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b359fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ = np.random.randn(512, 1024).astype(np.float32)\n",
    "B_ = np.random.randn(1024, 768).astype(np.float32)\n",
    "M, N, P = A_.shape[0], A_.shape[1], B_.shape[1]\n",
    "args = (np.int32(M), np.int32(N), np.int32(P))\n",
    "gpu_matrix_multiply(A_, B_, non_square_matmul, (16, 16), 2, 100, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646c596",
   "metadata": {},
   "source": [
    "Refer to the [solution](matmul_solution_non_square.c) if you get stuck."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
