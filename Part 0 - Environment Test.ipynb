{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Test\n",
    "\n",
    "This notebook is intended to run before the course to verify if the machines and environment works correctly.\n",
    "It can, and should be run by multiple people at the same time so that we make sure multiple people can work together.\n",
    "\n",
    "All cells should get executed and you should scroll down and see if the last message was printed. If not, report the problem to the lecturer or organizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing NVIDIA Drivers and OpenCL ICD\n",
    "\n",
    "To use OpenCL on NVIDIA GPUs, you need to install both the NVIDIA drivers and the OpenCL Installable Client Driver (ICD).\n",
    "\n",
    "### WSL\n",
    "\n",
    "DO NOT:\n",
    "- don't install any drivers in WSL2\n",
    "- don't install CUDA system wide with sudo apt\n",
    "\n",
    "You need to install WSL\n",
    "\n",
    "```wsl.exe --install```\n",
    "```wsl.exe --update```\n",
    "\n",
    "Create Ubuntu 24.04 LTS - you can do it in VS Code.\n",
    "\n",
    "```sudo apt update```\n",
    "```sudo apt upgrade```\n",
    "```sudo apt install -y build-essential```\n",
    "Install python\n",
    "Install conda\n",
    "It's not a good practice to use both conda and virtual environments - so we will use just conda envs.\n",
    "\n",
    "download installation script\n",
    "```wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh```\n",
    "run and answer the question - you can install it in user directory if working alone on a machine\n",
    "```bash Miniconda3-latest-Linux-x86_64.sh```\n",
    "\n",
    "create new conda env\n",
    "```conda create -n <env_name> python=<version>```\n",
    "As of 2025H2 choose python 3.11 for maximum compatibility.\n",
    "\n",
    "```\n",
    "conda create -n gpu python=3.11\n",
    "conda activate gpu\n",
    "```\n",
    "\n",
    "Look for latest cuda toolkit\n",
    "```conda search -c nvidia cuda-toolkit```\n",
    "Pycuda does not work with Cuda toolkit 13.0 on Python 3.11, so install older CUDA\n",
    "```conda install -c nvidia cuda-toolkit=12.8.1```\n",
    "\n",
    "Then use conda forge to build pycuda\n",
    "```conda install conda-forge::pycuda```\n",
    "\n",
    "check env with \n",
    "\n",
    "```env```\n",
    "\n",
    "Run Nvidia-smi to check your driver. This tool also reports CUDA Version but this is the latest supported CUDA version, not the installed version.\n",
    "```nvidia-smi```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test\n",
    "\n",
    "We won't go into details about the cell's meaning now - it will be explained during the course in the next notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import numpy as np\n",
    "\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "platform = cl.get_platforms()[0]\n",
    "\n",
    "ctx = cl.Context(\n",
    "    dev_type=cl.device_type.ALL, \n",
    "    properties=[(cl.context_properties.PLATFORM, platform)])    \n",
    "\n",
    "queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n",
    "    \n",
    "devices = ctx.get_info(cl.context_info.DEVICES)\n",
    "for d in devices:\n",
    "    print(f\"device={d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_gpu(function, n, queue, global_size, local_size, *args):\n",
    "    times = np.zeros(n)\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    function(queue, global_size, local_size, *args).wait()\n",
    "    \n",
    "    for i in range(n):\n",
    "        e = function(queue, global_size, local_size, *args)\n",
    "        e.wait()\n",
    "        elapsed = (e.profile.end - e.profile.start) * 1e-6\n",
    "        times[i] = elapsed\n",
    "\n",
    "    avg_ms = np.mean(times)\n",
    "    median_ms = np.median(times)\n",
    "    variance = np.var(times)\n",
    "    std = np.std(times)\n",
    "    print(f\"{function.function_name} took on average {avg_ms:.4f} ms, with median {median_ms:.4f} ms, variance {variance:.4f} ms, standard deviation {std:.4f} ms.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = np.int32(2**25)\n",
    "h_a = np.full(N, 1).astype(np.int32)\n",
    "h_b = np.full(N, 2).astype(np.int32)\n",
    "\n",
    "print(f\"Working with {len(h_a):,} elements with {h_a.nbytes:,} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required GPU buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = cl.mem_flags\n",
    "\n",
    "d_a = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_a)\n",
    "d_b = cl.Buffer(ctx, flags.READ_ONLY | flags.COPY_HOST_PTR, hostbuf=h_b)\n",
    "d_c = cl.Buffer(ctx, flags.WRITE_ONLY, h_a.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the kernel below to add elements from two arrays and write the result back to a third array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void add_vectors(__global const int *a, __global const int *b, __global int *c)\n",
    "{\n",
    "    int gid = get_global_id(0);\n",
    "    c[gid] = 2 * a[gid] + b[gid];\n",
    "}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create appropriate execution configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_work_size = (64,)\n",
    "global_work_size = (N,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute and profile the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_gpu(add_vectors, 20, \n",
    "            queue, \n",
    "            global_work_size, \n",
    "            local_work_size,\n",
    "            d_a,\n",
    "            d_b, \n",
    "            d_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h_c = np.zeros(N).astype(np.int32)\n",
    "cl.enqueue_copy(queue, h_c, d_c)\n",
    "\n",
    "def compute_linear_equations_cpu(a, b):\n",
    "    return 2 * a + b\n",
    "\n",
    "numpy_res = compute_linear_equations_cpu(h_a, h_b)\n",
    "np.testing.assert_array_equal(numpy_res, h_c)\n",
    "\n",
    "print(\"If this message got printed in the output cell then everything worked correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
